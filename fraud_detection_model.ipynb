{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T07:50:45.827826Z",
     "start_time": "2021-04-19T07:50:43.361129Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container{width : 100% ! important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from IPython.core.display import display, HTML\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier, LocalOutlierFactor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, IsolationForest\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, plot_confusion_matrix, plot_precision_recall_curve, plot_roc_curve, roc_auc_score\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.pandas.set_option('display.max_rows', None)\n",
    "pd.pandas.set_option('display.max_columns', None)\n",
    "display(HTML('<style>.container{width : 100% ! important;}</style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T07:50:53.877251Z",
     "start_time": "2021-04-19T07:50:45.832781Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('Train.csv')\n",
    "df_test = pd.read_csv('Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T07:50:53.923481Z",
     "start_time": "2021-04-19T07:50:53.878287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transaction ID</th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>465493</td>\n",
       "      <td>160933.0</td>\n",
       "      <td>-0.469383</td>\n",
       "      <td>0.944148</td>\n",
       "      <td>0.047223</td>\n",
       "      <td>-0.954715</td>\n",
       "      <td>1.374033</td>\n",
       "      <td>-0.094950</td>\n",
       "      <td>0.890572</td>\n",
       "      <td>0.109714</td>\n",
       "      <td>-0.198917</td>\n",
       "      <td>-1.621648</td>\n",
       "      <td>0.677703</td>\n",
       "      <td>0.405150</td>\n",
       "      <td>-0.212072</td>\n",
       "      <td>-1.531859</td>\n",
       "      <td>-1.100064</td>\n",
       "      <td>0.290751</td>\n",
       "      <td>0.735620</td>\n",
       "      <td>0.821074</td>\n",
       "      <td>0.407170</td>\n",
       "      <td>-0.011484</td>\n",
       "      <td>-0.164264</td>\n",
       "      <td>-0.388369</td>\n",
       "      <td>-0.284629</td>\n",
       "      <td>0.094658</td>\n",
       "      <td>0.111689</td>\n",
       "      <td>-0.108511</td>\n",
       "      <td>0.099751</td>\n",
       "      <td>0.152410</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>336662</td>\n",
       "      <td>80865.0</td>\n",
       "      <td>1.103816</td>\n",
       "      <td>-0.205496</td>\n",
       "      <td>1.350105</td>\n",
       "      <td>0.893491</td>\n",
       "      <td>-1.052071</td>\n",
       "      <td>-0.012629</td>\n",
       "      <td>-0.675143</td>\n",
       "      <td>0.178974</td>\n",
       "      <td>0.875989</td>\n",
       "      <td>-0.293025</td>\n",
       "      <td>-0.188309</td>\n",
       "      <td>0.731998</td>\n",
       "      <td>0.153651</td>\n",
       "      <td>-0.394624</td>\n",
       "      <td>0.408450</td>\n",
       "      <td>-0.232430</td>\n",
       "      <td>0.211982</td>\n",
       "      <td>-0.812418</td>\n",
       "      <td>-0.524743</td>\n",
       "      <td>-0.118579</td>\n",
       "      <td>-0.036948</td>\n",
       "      <td>0.116054</td>\n",
       "      <td>0.093842</td>\n",
       "      <td>0.454146</td>\n",
       "      <td>0.142234</td>\n",
       "      <td>0.324634</td>\n",
       "      <td>0.033858</td>\n",
       "      <td>0.028680</td>\n",
       "      <td>14.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>357179</td>\n",
       "      <td>104436.0</td>\n",
       "      <td>0.138446</td>\n",
       "      <td>1.217584</td>\n",
       "      <td>-0.293554</td>\n",
       "      <td>0.483069</td>\n",
       "      <td>1.180806</td>\n",
       "      <td>-1.208567</td>\n",
       "      <td>1.564626</td>\n",
       "      <td>-0.480288</td>\n",
       "      <td>0.511007</td>\n",
       "      <td>-0.681823</td>\n",
       "      <td>-0.123528</td>\n",
       "      <td>-2.975377</td>\n",
       "      <td>0.889293</td>\n",
       "      <td>2.238183</td>\n",
       "      <td>-0.890180</td>\n",
       "      <td>-0.430066</td>\n",
       "      <td>0.239167</td>\n",
       "      <td>-0.007760</td>\n",
       "      <td>-0.829333</td>\n",
       "      <td>-0.296091</td>\n",
       "      <td>0.050834</td>\n",
       "      <td>0.392587</td>\n",
       "      <td>-0.232261</td>\n",
       "      <td>-0.166938</td>\n",
       "      <td>0.177935</td>\n",
       "      <td>-0.589114</td>\n",
       "      <td>0.009673</td>\n",
       "      <td>0.024138</td>\n",
       "      <td>19.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>262371</td>\n",
       "      <td>49241.0</td>\n",
       "      <td>0.991295</td>\n",
       "      <td>-0.645264</td>\n",
       "      <td>1.032155</td>\n",
       "      <td>0.331717</td>\n",
       "      <td>-1.216554</td>\n",
       "      <td>-0.317276</td>\n",
       "      <td>-0.430301</td>\n",
       "      <td>-0.033865</td>\n",
       "      <td>1.035865</td>\n",
       "      <td>-0.535720</td>\n",
       "      <td>-0.620529</td>\n",
       "      <td>1.128655</td>\n",
       "      <td>1.029532</td>\n",
       "      <td>-0.735555</td>\n",
       "      <td>-0.401473</td>\n",
       "      <td>-0.152193</td>\n",
       "      <td>0.087158</td>\n",
       "      <td>-0.740995</td>\n",
       "      <td>0.424010</td>\n",
       "      <td>0.222022</td>\n",
       "      <td>-0.152953</td>\n",
       "      <td>-0.389418</td>\n",
       "      <td>-0.033484</td>\n",
       "      <td>0.500995</td>\n",
       "      <td>0.129980</td>\n",
       "      <td>0.926788</td>\n",
       "      <td>-0.048619</td>\n",
       "      <td>0.036531</td>\n",
       "      <td>123.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>234477</td>\n",
       "      <td>36772.0</td>\n",
       "      <td>-0.717427</td>\n",
       "      <td>-3.468358</td>\n",
       "      <td>0.084280</td>\n",
       "      <td>0.955867</td>\n",
       "      <td>-1.711766</td>\n",
       "      <td>1.232775</td>\n",
       "      <td>0.290128</td>\n",
       "      <td>0.275804</td>\n",
       "      <td>0.840400</td>\n",
       "      <td>-0.725275</td>\n",
       "      <td>1.259676</td>\n",
       "      <td>1.165983</td>\n",
       "      <td>-0.529007</td>\n",
       "      <td>0.016380</td>\n",
       "      <td>-0.364619</td>\n",
       "      <td>-0.328462</td>\n",
       "      <td>0.495466</td>\n",
       "      <td>-0.790906</td>\n",
       "      <td>-0.331250</td>\n",
       "      <td>1.718714</td>\n",
       "      <td>0.430734</td>\n",
       "      <td>-0.603584</td>\n",
       "      <td>-0.682557</td>\n",
       "      <td>-0.184120</td>\n",
       "      <td>-0.396997</td>\n",
       "      <td>0.855515</td>\n",
       "      <td>-0.180882</td>\n",
       "      <td>0.158518</td>\n",
       "      <td>935.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Transaction ID      Time        V1        V2        V3        V4        V5  \\\n",
       "0          465493  160933.0 -0.469383  0.944148  0.047223 -0.954715  1.374033   \n",
       "1          336662   80865.0  1.103816 -0.205496  1.350105  0.893491 -1.052071   \n",
       "2          357179  104436.0  0.138446  1.217584 -0.293554  0.483069  1.180806   \n",
       "3          262371   49241.0  0.991295 -0.645264  1.032155  0.331717 -1.216554   \n",
       "4          234477   36772.0 -0.717427 -3.468358  0.084280  0.955867 -1.711766   \n",
       "\n",
       "         V6        V7        V8        V9       V10       V11       V12  \\\n",
       "0 -0.094950  0.890572  0.109714 -0.198917 -1.621648  0.677703  0.405150   \n",
       "1 -0.012629 -0.675143  0.178974  0.875989 -0.293025 -0.188309  0.731998   \n",
       "2 -1.208567  1.564626 -0.480288  0.511007 -0.681823 -0.123528 -2.975377   \n",
       "3 -0.317276 -0.430301 -0.033865  1.035865 -0.535720 -0.620529  1.128655   \n",
       "4  1.232775  0.290128  0.275804  0.840400 -0.725275  1.259676  1.165983   \n",
       "\n",
       "        V13       V14       V15       V16       V17       V18       V19  \\\n",
       "0 -0.212072 -1.531859 -1.100064  0.290751  0.735620  0.821074  0.407170   \n",
       "1  0.153651 -0.394624  0.408450 -0.232430  0.211982 -0.812418 -0.524743   \n",
       "2  0.889293  2.238183 -0.890180 -0.430066  0.239167 -0.007760 -0.829333   \n",
       "3  1.029532 -0.735555 -0.401473 -0.152193  0.087158 -0.740995  0.424010   \n",
       "4 -0.529007  0.016380 -0.364619 -0.328462  0.495466 -0.790906 -0.331250   \n",
       "\n",
       "        V20       V21       V22       V23       V24       V25       V26  \\\n",
       "0 -0.011484 -0.164264 -0.388369 -0.284629  0.094658  0.111689 -0.108511   \n",
       "1 -0.118579 -0.036948  0.116054  0.093842  0.454146  0.142234  0.324634   \n",
       "2 -0.296091  0.050834  0.392587 -0.232261 -0.166938  0.177935 -0.589114   \n",
       "3  0.222022 -0.152953 -0.389418 -0.033484  0.500995  0.129980  0.926788   \n",
       "4  1.718714  0.430734 -0.603584 -0.682557 -0.184120 -0.396997  0.855515   \n",
       "\n",
       "        V27       V28  Amount  Class  \n",
       "0  0.099751  0.152410    7.00      0  \n",
       "1  0.033858  0.028680   14.99      0  \n",
       "2  0.009673  0.024138   19.99      0  \n",
       "3 -0.048619  0.036531  123.06      0  \n",
       "4 -0.180882  0.158518  935.95      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T07:50:53.955367Z",
     "start_time": "2021-04-19T07:50:53.925447Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df_train.drop(['Transaction ID', 'Time', 'Class'], axis = 1)\n",
    "y = df_train['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T07:50:53.971324Z",
     "start_time": "2021-04-19T07:50:53.957362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    227451\n",
       "1       394\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T07:50:54.192415Z",
     "start_time": "2021-04-19T07:50:53.973320Z"
    }
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_scaled = pd.DataFrame(ss.fit_transform(X), columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T07:50:54.223250Z",
     "start_time": "2021-04-19T07:50:54.194444Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.239399</td>\n",
       "      <td>0.572132</td>\n",
       "      <td>0.031106</td>\n",
       "      <td>-0.673864</td>\n",
       "      <td>0.992997</td>\n",
       "      <td>-0.070891</td>\n",
       "      <td>0.712090</td>\n",
       "      <td>0.092261</td>\n",
       "      <td>-0.182178</td>\n",
       "      <td>-1.487675</td>\n",
       "      <td>0.665150</td>\n",
       "      <td>0.406405</td>\n",
       "      <td>-0.212355</td>\n",
       "      <td>-1.602484</td>\n",
       "      <td>-1.203476</td>\n",
       "      <td>0.332538</td>\n",
       "      <td>0.869787</td>\n",
       "      <td>0.979926</td>\n",
       "      <td>0.499258</td>\n",
       "      <td>-0.014268</td>\n",
       "      <td>-0.220934</td>\n",
       "      <td>-0.535007</td>\n",
       "      <td>-0.453051</td>\n",
       "      <td>0.156089</td>\n",
       "      <td>0.211911</td>\n",
       "      <td>-0.223308</td>\n",
       "      <td>0.245859</td>\n",
       "      <td>0.469559</td>\n",
       "      <td>-0.319215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.563285</td>\n",
       "      <td>-0.125140</td>\n",
       "      <td>0.887696</td>\n",
       "      <td>0.631236</td>\n",
       "      <td>-0.760256</td>\n",
       "      <td>-0.009155</td>\n",
       "      <td>-0.541303</td>\n",
       "      <td>0.150137</td>\n",
       "      <td>0.795386</td>\n",
       "      <td>-0.268399</td>\n",
       "      <td>-0.183900</td>\n",
       "      <td>0.734074</td>\n",
       "      <td>0.155281</td>\n",
       "      <td>-0.412837</td>\n",
       "      <td>0.445573</td>\n",
       "      <td>-0.265366</td>\n",
       "      <td>0.250391</td>\n",
       "      <td>-0.970527</td>\n",
       "      <td>-0.644530</td>\n",
       "      <td>-0.151940</td>\n",
       "      <td>-0.048424</td>\n",
       "      <td>0.159890</td>\n",
       "      <td>0.150983</td>\n",
       "      <td>0.748856</td>\n",
       "      <td>0.270458</td>\n",
       "      <td>0.675860</td>\n",
       "      <td>0.083143</td>\n",
       "      <td>0.088938</td>\n",
       "      <td>-0.287875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.070730</td>\n",
       "      <td>0.737974</td>\n",
       "      <td>-0.192940</td>\n",
       "      <td>0.341419</td>\n",
       "      <td>0.853360</td>\n",
       "      <td>-0.906032</td>\n",
       "      <td>1.251687</td>\n",
       "      <td>-0.400765</td>\n",
       "      <td>0.463456</td>\n",
       "      <td>-0.625198</td>\n",
       "      <td>-0.120388</td>\n",
       "      <td>-2.982610</td>\n",
       "      <td>0.894769</td>\n",
       "      <td>2.341307</td>\n",
       "      <td>-0.974039</td>\n",
       "      <td>-0.491229</td>\n",
       "      <td>0.282548</td>\n",
       "      <td>-0.009734</td>\n",
       "      <td>-1.018371</td>\n",
       "      <td>-0.380134</td>\n",
       "      <td>0.070518</td>\n",
       "      <td>0.540844</td>\n",
       "      <td>-0.369473</td>\n",
       "      <td>-0.275260</td>\n",
       "      <td>0.338888</td>\n",
       "      <td>-1.220992</td>\n",
       "      <td>0.023420</td>\n",
       "      <td>0.074967</td>\n",
       "      <td>-0.268262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.505874</td>\n",
       "      <td>-0.391864</td>\n",
       "      <td>0.678657</td>\n",
       "      <td>0.234543</td>\n",
       "      <td>-0.879122</td>\n",
       "      <td>-0.237621</td>\n",
       "      <td>-0.345301</td>\n",
       "      <td>-0.027719</td>\n",
       "      <td>0.940785</td>\n",
       "      <td>-0.491119</td>\n",
       "      <td>-0.607654</td>\n",
       "      <td>1.131726</td>\n",
       "      <td>1.035743</td>\n",
       "      <td>-0.769480</td>\n",
       "      <td>-0.439804</td>\n",
       "      <td>-0.173669</td>\n",
       "      <td>0.102740</td>\n",
       "      <td>-0.885245</td>\n",
       "      <td>0.519927</td>\n",
       "      <td>0.285905</td>\n",
       "      <td>-0.205608</td>\n",
       "      <td>-0.536452</td>\n",
       "      <td>-0.052227</td>\n",
       "      <td>0.826107</td>\n",
       "      <td>0.246970</td>\n",
       "      <td>1.925872</td>\n",
       "      <td>-0.120529</td>\n",
       "      <td>0.113089</td>\n",
       "      <td>0.136026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.365957</td>\n",
       "      <td>-2.104103</td>\n",
       "      <td>0.055470</td>\n",
       "      <td>0.675283</td>\n",
       "      <td>-1.236993</td>\n",
       "      <td>0.924819</td>\n",
       "      <td>0.231420</td>\n",
       "      <td>0.231051</td>\n",
       "      <td>0.763020</td>\n",
       "      <td>-0.665074</td>\n",
       "      <td>1.235726</td>\n",
       "      <td>1.169148</td>\n",
       "      <td>-0.530948</td>\n",
       "      <td>0.017109</td>\n",
       "      <td>-0.399517</td>\n",
       "      <td>-0.375114</td>\n",
       "      <td>0.585716</td>\n",
       "      <td>-0.944840</td>\n",
       "      <td>-0.407046</td>\n",
       "      <td>2.209915</td>\n",
       "      <td>0.585272</td>\n",
       "      <td>-0.831489</td>\n",
       "      <td>-1.088139</td>\n",
       "      <td>-0.303593</td>\n",
       "      <td>-0.763130</td>\n",
       "      <td>1.777916</td>\n",
       "      <td>-0.447138</td>\n",
       "      <td>0.488348</td>\n",
       "      <td>3.324557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -0.239399  0.572132  0.031106 -0.673864  0.992997 -0.070891  0.712090   \n",
       "1  0.563285 -0.125140  0.887696  0.631236 -0.760256 -0.009155 -0.541303   \n",
       "2  0.070730  0.737974 -0.192940  0.341419  0.853360 -0.906032  1.251687   \n",
       "3  0.505874 -0.391864  0.678657  0.234543 -0.879122 -0.237621 -0.345301   \n",
       "4 -0.365957 -2.104103  0.055470  0.675283 -1.236993  0.924819  0.231420   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.092261 -0.182178 -1.487675  0.665150  0.406405 -0.212355 -1.602484   \n",
       "1  0.150137  0.795386 -0.268399 -0.183900  0.734074  0.155281 -0.412837   \n",
       "2 -0.400765  0.463456 -0.625198 -0.120388 -2.982610  0.894769  2.341307   \n",
       "3 -0.027719  0.940785 -0.491119 -0.607654  1.131726  1.035743 -0.769480   \n",
       "4  0.231051  0.763020 -0.665074  1.235726  1.169148 -0.530948  0.017109   \n",
       "\n",
       "        V15       V16       V17       V18       V19       V20       V21  \\\n",
       "0 -1.203476  0.332538  0.869787  0.979926  0.499258 -0.014268 -0.220934   \n",
       "1  0.445573 -0.265366  0.250391 -0.970527 -0.644530 -0.151940 -0.048424   \n",
       "2 -0.974039 -0.491229  0.282548 -0.009734 -1.018371 -0.380134  0.070518   \n",
       "3 -0.439804 -0.173669  0.102740 -0.885245  0.519927  0.285905 -0.205608   \n",
       "4 -0.399517 -0.375114  0.585716 -0.944840 -0.407046  2.209915  0.585272   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0 -0.535007 -0.453051  0.156089  0.211911 -0.223308  0.245859  0.469559   \n",
       "1  0.159890  0.150983  0.748856  0.270458  0.675860  0.083143  0.088938   \n",
       "2  0.540844 -0.369473 -0.275260  0.338888 -1.220992  0.023420  0.074967   \n",
       "3 -0.536452 -0.052227  0.826107  0.246970  1.925872 -0.120529  0.113089   \n",
       "4 -0.831489 -1.088139 -0.303593 -0.763130  1.777916 -0.447138  0.488348   \n",
       "\n",
       "     Amount  \n",
       "0 -0.319215  \n",
       "1 -0.287875  \n",
       "2 -0.268262  \n",
       "3  0.136026  \n",
       "4  3.324557  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T07:50:54.381566Z",
     "start_time": "2021-04-19T07:50:54.225204Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size = 0.2, random_state = 17, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T07:50:54.396967Z",
     "start_time": "2021-04-19T07:50:54.383527Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((182276, 29), (45569, 29), (182276,), (45569,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T12:42:17.795978Z",
     "start_time": "2021-04-18T12:42:15.825119Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(verbose=2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(verbose = 2)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T12:42:23.000557Z",
     "start_time": "2021-04-18T12:42:21.817666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Train f1_score:  0.7225325884543761\n",
      "Validation f1_score:  0.759124087591241\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Train precision:  0.8738738738738738\n",
      "Validation precision:  0.896551724137931\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Train recall:  0.6158730158730159\n",
      "Validation recall:  0.6582278481012658\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x2dc9f2be0a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train_pred_lr = lr.predict(X_train)\n",
    "y_val_pred_lr = lr.predict(X_val)\n",
    "print('Logistic Regression:')\n",
    "print('-' * 100)\n",
    "print('Train f1_score: ', f1_score(y_train, y_train_pred_lr))\n",
    "print('Validation f1_score: ', f1_score(y_val, y_val_pred_lr))\n",
    "print('-' * 100)\n",
    "print('Train precision: ', precision_score(y_train, y_train_pred_lr))\n",
    "print('Validation precision: ', precision_score(y_val, y_val_pred_lr))\n",
    "print('-' * 100)\n",
    "print('Train recall: ', recall_score(y_train, y_train_pred_lr))\n",
    "print('Validation recall: ', recall_score(y_val, y_val_pred_lr))\n",
    "print('-' * 100)\n",
    "plot_confusion_matrix(lr, X_val, y_val)\n",
    "plot_roc_curve(lr, X_val, y_val)\n",
    "plot_precision_recall_curve(lr, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T12:48:42.880389Z",
     "start_time": "2021-04-18T12:43:33.836804Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[CV] splitter=random, min_samples_split=29, min_samples_leaf=38, max_depth=35, criterion=gini \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  splitter=random, min_samples_split=29, min_samples_leaf=38, max_depth=35, criterion=gini, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=29, min_samples_leaf=38, max_depth=35, criterion=gini \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  splitter=random, min_samples_split=29, min_samples_leaf=38, max_depth=35, criterion=gini, total=   0.5s\n",
      "[CV] splitter=random, min_samples_split=29, min_samples_leaf=38, max_depth=35, criterion=gini \n",
      "[CV]  splitter=random, min_samples_split=29, min_samples_leaf=38, max_depth=35, criterion=gini, total=   0.5s\n",
      "[CV] splitter=random, min_samples_split=29, min_samples_leaf=38, max_depth=35, criterion=gini \n",
      "[CV]  splitter=random, min_samples_split=29, min_samples_leaf=38, max_depth=35, criterion=gini, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=29, min_samples_leaf=38, max_depth=35, criterion=gini \n",
      "[CV]  splitter=random, min_samples_split=29, min_samples_leaf=38, max_depth=35, criterion=gini, total=   0.5s\n",
      "[CV] splitter=random, min_samples_split=29, min_samples_leaf=38, max_depth=35, criterion=gini \n",
      "[CV]  splitter=random, min_samples_split=29, min_samples_leaf=38, max_depth=35, criterion=gini, total=   0.5s\n",
      "[CV] splitter=random, min_samples_split=29, min_samples_leaf=38, max_depth=35, criterion=gini \n",
      "[CV]  splitter=random, min_samples_split=29, min_samples_leaf=38, max_depth=35, criterion=gini, total=   0.5s\n",
      "[CV] splitter=random, min_samples_split=29, min_samples_leaf=38, max_depth=35, criterion=gini \n",
      "[CV]  splitter=random, min_samples_split=29, min_samples_leaf=38, max_depth=35, criterion=gini, total=   0.5s\n",
      "[CV] splitter=random, min_samples_split=29, min_samples_leaf=38, max_depth=35, criterion=gini \n",
      "[CV]  splitter=random, min_samples_split=29, min_samples_leaf=38, max_depth=35, criterion=gini, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=29, min_samples_leaf=38, max_depth=35, criterion=gini \n",
      "[CV]  splitter=random, min_samples_split=29, min_samples_leaf=38, max_depth=35, criterion=gini, total=   0.5s\n",
      "[CV] splitter=random, min_samples_split=44, min_samples_leaf=48, max_depth=55, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=44, min_samples_leaf=48, max_depth=55, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=44, min_samples_leaf=48, max_depth=55, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=44, min_samples_leaf=48, max_depth=55, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=44, min_samples_leaf=48, max_depth=55, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=44, min_samples_leaf=48, max_depth=55, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=44, min_samples_leaf=48, max_depth=55, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=44, min_samples_leaf=48, max_depth=55, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=44, min_samples_leaf=48, max_depth=55, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=44, min_samples_leaf=48, max_depth=55, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=44, min_samples_leaf=48, max_depth=55, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=44, min_samples_leaf=48, max_depth=55, criterion=entropy, total=   0.3s\n",
      "[CV] splitter=random, min_samples_split=44, min_samples_leaf=48, max_depth=55, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=44, min_samples_leaf=48, max_depth=55, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=44, min_samples_leaf=48, max_depth=55, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=44, min_samples_leaf=48, max_depth=55, criterion=entropy, total=   0.5s\n",
      "[CV] splitter=random, min_samples_split=44, min_samples_leaf=48, max_depth=55, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=44, min_samples_leaf=48, max_depth=55, criterion=entropy, total=   0.3s\n",
      "[CV] splitter=random, min_samples_split=44, min_samples_leaf=48, max_depth=55, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=44, min_samples_leaf=48, max_depth=55, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=65, min_samples_leaf=36, max_depth=60, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=65, min_samples_leaf=36, max_depth=60, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=65, min_samples_leaf=36, max_depth=60, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=65, min_samples_leaf=36, max_depth=60, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=65, min_samples_leaf=36, max_depth=60, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=65, min_samples_leaf=36, max_depth=60, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=65, min_samples_leaf=36, max_depth=60, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=65, min_samples_leaf=36, max_depth=60, criterion=entropy, total=   0.3s\n",
      "[CV] splitter=random, min_samples_split=65, min_samples_leaf=36, max_depth=60, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=65, min_samples_leaf=36, max_depth=60, criterion=entropy, total=   0.3s\n",
      "[CV] splitter=random, min_samples_split=65, min_samples_leaf=36, max_depth=60, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=65, min_samples_leaf=36, max_depth=60, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=65, min_samples_leaf=36, max_depth=60, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=65, min_samples_leaf=36, max_depth=60, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=65, min_samples_leaf=36, max_depth=60, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=65, min_samples_leaf=36, max_depth=60, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=65, min_samples_leaf=36, max_depth=60, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=65, min_samples_leaf=36, max_depth=60, criterion=entropy, total=   0.5s\n",
      "[CV] splitter=random, min_samples_split=65, min_samples_leaf=36, max_depth=60, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=65, min_samples_leaf=36, max_depth=60, criterion=entropy, total=   0.3s\n",
      "[CV] splitter=best, min_samples_split=61, min_samples_leaf=5, max_depth=70, criterion=gini \n",
      "[CV]  splitter=best, min_samples_split=61, min_samples_leaf=5, max_depth=70, criterion=gini, total=   9.8s\n",
      "[CV] splitter=best, min_samples_split=61, min_samples_leaf=5, max_depth=70, criterion=gini \n",
      "[CV]  splitter=best, min_samples_split=61, min_samples_leaf=5, max_depth=70, criterion=gini, total=   7.8s\n",
      "[CV] splitter=best, min_samples_split=61, min_samples_leaf=5, max_depth=70, criterion=gini \n",
      "[CV]  splitter=best, min_samples_split=61, min_samples_leaf=5, max_depth=70, criterion=gini, total=   7.7s\n",
      "[CV] splitter=best, min_samples_split=61, min_samples_leaf=5, max_depth=70, criterion=gini \n",
      "[CV]  splitter=best, min_samples_split=61, min_samples_leaf=5, max_depth=70, criterion=gini, total=   7.8s\n",
      "[CV] splitter=best, min_samples_split=61, min_samples_leaf=5, max_depth=70, criterion=gini \n",
      "[CV]  splitter=best, min_samples_split=61, min_samples_leaf=5, max_depth=70, criterion=gini, total=   8.2s\n",
      "[CV] splitter=best, min_samples_split=61, min_samples_leaf=5, max_depth=70, criterion=gini \n",
      "[CV]  splitter=best, min_samples_split=61, min_samples_leaf=5, max_depth=70, criterion=gini, total=   8.9s\n",
      "[CV] splitter=best, min_samples_split=61, min_samples_leaf=5, max_depth=70, criterion=gini \n",
      "[CV]  splitter=best, min_samples_split=61, min_samples_leaf=5, max_depth=70, criterion=gini, total=   8.2s\n",
      "[CV] splitter=best, min_samples_split=61, min_samples_leaf=5, max_depth=70, criterion=gini \n",
      "[CV]  splitter=best, min_samples_split=61, min_samples_leaf=5, max_depth=70, criterion=gini, total=   9.1s\n",
      "[CV] splitter=best, min_samples_split=61, min_samples_leaf=5, max_depth=70, criterion=gini \n",
      "[CV]  splitter=best, min_samples_split=61, min_samples_leaf=5, max_depth=70, criterion=gini, total=   7.4s\n",
      "[CV] splitter=best, min_samples_split=61, min_samples_leaf=5, max_depth=70, criterion=gini \n",
      "[CV]  splitter=best, min_samples_split=61, min_samples_leaf=5, max_depth=70, criterion=gini, total=   7.7s\n",
      "[CV] splitter=random, min_samples_split=59, min_samples_leaf=12, max_depth=35, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=59, min_samples_leaf=12, max_depth=35, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=59, min_samples_leaf=12, max_depth=35, criterion=entropy \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  splitter=random, min_samples_split=59, min_samples_leaf=12, max_depth=35, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=59, min_samples_leaf=12, max_depth=35, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=59, min_samples_leaf=12, max_depth=35, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=59, min_samples_leaf=12, max_depth=35, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=59, min_samples_leaf=12, max_depth=35, criterion=entropy, total=   0.3s\n",
      "[CV] splitter=random, min_samples_split=59, min_samples_leaf=12, max_depth=35, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=59, min_samples_leaf=12, max_depth=35, criterion=entropy, total=   0.3s\n",
      "[CV] splitter=random, min_samples_split=59, min_samples_leaf=12, max_depth=35, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=59, min_samples_leaf=12, max_depth=35, criterion=entropy, total=   0.3s\n",
      "[CV] splitter=random, min_samples_split=59, min_samples_leaf=12, max_depth=35, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=59, min_samples_leaf=12, max_depth=35, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=59, min_samples_leaf=12, max_depth=35, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=59, min_samples_leaf=12, max_depth=35, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=59, min_samples_leaf=12, max_depth=35, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=59, min_samples_leaf=12, max_depth=35, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=59, min_samples_leaf=12, max_depth=35, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=59, min_samples_leaf=12, max_depth=35, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=84, min_samples_leaf=49, max_depth=50, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=84, min_samples_leaf=49, max_depth=50, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=84, min_samples_leaf=49, max_depth=50, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=84, min_samples_leaf=49, max_depth=50, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=84, min_samples_leaf=49, max_depth=50, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=84, min_samples_leaf=49, max_depth=50, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=84, min_samples_leaf=49, max_depth=50, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=84, min_samples_leaf=49, max_depth=50, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=84, min_samples_leaf=49, max_depth=50, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=84, min_samples_leaf=49, max_depth=50, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=84, min_samples_leaf=49, max_depth=50, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=84, min_samples_leaf=49, max_depth=50, criterion=entropy, total=   0.3s\n",
      "[CV] splitter=random, min_samples_split=84, min_samples_leaf=49, max_depth=50, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=84, min_samples_leaf=49, max_depth=50, criterion=entropy, total=   0.3s\n",
      "[CV] splitter=random, min_samples_split=84, min_samples_leaf=49, max_depth=50, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=84, min_samples_leaf=49, max_depth=50, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=84, min_samples_leaf=49, max_depth=50, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=84, min_samples_leaf=49, max_depth=50, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=84, min_samples_leaf=49, max_depth=50, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=84, min_samples_leaf=49, max_depth=50, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=best, min_samples_split=39, min_samples_leaf=19, max_depth=90, criterion=entropy \n",
      "[CV]  splitter=best, min_samples_split=39, min_samples_leaf=19, max_depth=90, criterion=entropy, total=   4.8s\n",
      "[CV] splitter=best, min_samples_split=39, min_samples_leaf=19, max_depth=90, criterion=entropy \n",
      "[CV]  splitter=best, min_samples_split=39, min_samples_leaf=19, max_depth=90, criterion=entropy, total=   6.0s\n",
      "[CV] splitter=best, min_samples_split=39, min_samples_leaf=19, max_depth=90, criterion=entropy \n",
      "[CV]  splitter=best, min_samples_split=39, min_samples_leaf=19, max_depth=90, criterion=entropy, total=   5.8s\n",
      "[CV] splitter=best, min_samples_split=39, min_samples_leaf=19, max_depth=90, criterion=entropy \n",
      "[CV]  splitter=best, min_samples_split=39, min_samples_leaf=19, max_depth=90, criterion=entropy, total=   6.1s\n",
      "[CV] splitter=best, min_samples_split=39, min_samples_leaf=19, max_depth=90, criterion=entropy \n",
      "[CV]  splitter=best, min_samples_split=39, min_samples_leaf=19, max_depth=90, criterion=entropy, total=   5.7s\n",
      "[CV] splitter=best, min_samples_split=39, min_samples_leaf=19, max_depth=90, criterion=entropy \n",
      "[CV]  splitter=best, min_samples_split=39, min_samples_leaf=19, max_depth=90, criterion=entropy, total=   5.4s\n",
      "[CV] splitter=best, min_samples_split=39, min_samples_leaf=19, max_depth=90, criterion=entropy \n",
      "[CV]  splitter=best, min_samples_split=39, min_samples_leaf=19, max_depth=90, criterion=entropy, total=   5.9s\n",
      "[CV] splitter=best, min_samples_split=39, min_samples_leaf=19, max_depth=90, criterion=entropy \n",
      "[CV]  splitter=best, min_samples_split=39, min_samples_leaf=19, max_depth=90, criterion=entropy, total=   5.6s\n",
      "[CV] splitter=best, min_samples_split=39, min_samples_leaf=19, max_depth=90, criterion=entropy \n",
      "[CV]  splitter=best, min_samples_split=39, min_samples_leaf=19, max_depth=90, criterion=entropy, total=   4.9s\n",
      "[CV] splitter=best, min_samples_split=39, min_samples_leaf=19, max_depth=90, criterion=entropy \n",
      "[CV]  splitter=best, min_samples_split=39, min_samples_leaf=19, max_depth=90, criterion=entropy, total=   5.9s\n",
      "[CV] splitter=random, min_samples_split=56, min_samples_leaf=32, max_depth=100, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=56, min_samples_leaf=32, max_depth=100, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=56, min_samples_leaf=32, max_depth=100, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=56, min_samples_leaf=32, max_depth=100, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=56, min_samples_leaf=32, max_depth=100, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=56, min_samples_leaf=32, max_depth=100, criterion=entropy, total=   0.3s\n",
      "[CV] splitter=random, min_samples_split=56, min_samples_leaf=32, max_depth=100, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=56, min_samples_leaf=32, max_depth=100, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=56, min_samples_leaf=32, max_depth=100, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=56, min_samples_leaf=32, max_depth=100, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=56, min_samples_leaf=32, max_depth=100, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=56, min_samples_leaf=32, max_depth=100, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=56, min_samples_leaf=32, max_depth=100, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=56, min_samples_leaf=32, max_depth=100, criterion=entropy, total=   0.5s\n",
      "[CV] splitter=random, min_samples_split=56, min_samples_leaf=32, max_depth=100, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=56, min_samples_leaf=32, max_depth=100, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=random, min_samples_split=56, min_samples_leaf=32, max_depth=100, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=56, min_samples_leaf=32, max_depth=100, criterion=entropy, total=   0.3s\n",
      "[CV] splitter=random, min_samples_split=56, min_samples_leaf=32, max_depth=100, criterion=entropy \n",
      "[CV]  splitter=random, min_samples_split=56, min_samples_leaf=32, max_depth=100, criterion=entropy, total=   0.4s\n",
      "[CV] splitter=best, min_samples_split=79, min_samples_leaf=10, max_depth=60, criterion=entropy \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  splitter=best, min_samples_split=79, min_samples_leaf=10, max_depth=60, criterion=entropy, total=   5.3s\n",
      "[CV] splitter=best, min_samples_split=79, min_samples_leaf=10, max_depth=60, criterion=entropy \n",
      "[CV]  splitter=best, min_samples_split=79, min_samples_leaf=10, max_depth=60, criterion=entropy, total=   6.4s\n",
      "[CV] splitter=best, min_samples_split=79, min_samples_leaf=10, max_depth=60, criterion=entropy \n",
      "[CV]  splitter=best, min_samples_split=79, min_samples_leaf=10, max_depth=60, criterion=entropy, total=   6.2s\n",
      "[CV] splitter=best, min_samples_split=79, min_samples_leaf=10, max_depth=60, criterion=entropy \n",
      "[CV]  splitter=best, min_samples_split=79, min_samples_leaf=10, max_depth=60, criterion=entropy, total=   6.8s\n",
      "[CV] splitter=best, min_samples_split=79, min_samples_leaf=10, max_depth=60, criterion=entropy \n",
      "[CV]  splitter=best, min_samples_split=79, min_samples_leaf=10, max_depth=60, criterion=entropy, total=   6.8s\n",
      "[CV] splitter=best, min_samples_split=79, min_samples_leaf=10, max_depth=60, criterion=entropy \n",
      "[CV]  splitter=best, min_samples_split=79, min_samples_leaf=10, max_depth=60, criterion=entropy, total=   5.4s\n",
      "[CV] splitter=best, min_samples_split=79, min_samples_leaf=10, max_depth=60, criterion=entropy \n",
      "[CV]  splitter=best, min_samples_split=79, min_samples_leaf=10, max_depth=60, criterion=entropy, total=   5.9s\n",
      "[CV] splitter=best, min_samples_split=79, min_samples_leaf=10, max_depth=60, criterion=entropy \n",
      "[CV]  splitter=best, min_samples_split=79, min_samples_leaf=10, max_depth=60, criterion=entropy, total=   5.7s\n",
      "[CV] splitter=best, min_samples_split=79, min_samples_leaf=10, max_depth=60, criterion=entropy \n",
      "[CV]  splitter=best, min_samples_split=79, min_samples_leaf=10, max_depth=60, criterion=entropy, total=   4.9s\n",
      "[CV] splitter=best, min_samples_split=79, min_samples_leaf=10, max_depth=60, criterion=entropy \n",
      "[CV]  splitter=best, min_samples_split=79, min_samples_leaf=10, max_depth=60, criterion=entropy, total=   5.7s\n",
      "[CV] splitter=best, min_samples_split=59, min_samples_leaf=16, max_depth=45, criterion=gini \n",
      "[CV]  splitter=best, min_samples_split=59, min_samples_leaf=16, max_depth=45, criterion=gini, total=   9.1s\n",
      "[CV] splitter=best, min_samples_split=59, min_samples_leaf=16, max_depth=45, criterion=gini \n",
      "[CV]  splitter=best, min_samples_split=59, min_samples_leaf=16, max_depth=45, criterion=gini, total=   7.7s\n",
      "[CV] splitter=best, min_samples_split=59, min_samples_leaf=16, max_depth=45, criterion=gini \n",
      "[CV]  splitter=best, min_samples_split=59, min_samples_leaf=16, max_depth=45, criterion=gini, total=   7.7s\n",
      "[CV] splitter=best, min_samples_split=59, min_samples_leaf=16, max_depth=45, criterion=gini \n",
      "[CV]  splitter=best, min_samples_split=59, min_samples_leaf=16, max_depth=45, criterion=gini, total=   7.2s\n",
      "[CV] splitter=best, min_samples_split=59, min_samples_leaf=16, max_depth=45, criterion=gini \n",
      "[CV]  splitter=best, min_samples_split=59, min_samples_leaf=16, max_depth=45, criterion=gini, total=   8.1s\n",
      "[CV] splitter=best, min_samples_split=59, min_samples_leaf=16, max_depth=45, criterion=gini \n",
      "[CV]  splitter=best, min_samples_split=59, min_samples_leaf=16, max_depth=45, criterion=gini, total=   8.4s\n",
      "[CV] splitter=best, min_samples_split=59, min_samples_leaf=16, max_depth=45, criterion=gini \n",
      "[CV]  splitter=best, min_samples_split=59, min_samples_leaf=16, max_depth=45, criterion=gini, total=   7.7s\n",
      "[CV] splitter=best, min_samples_split=59, min_samples_leaf=16, max_depth=45, criterion=gini \n",
      "[CV]  splitter=best, min_samples_split=59, min_samples_leaf=16, max_depth=45, criterion=gini, total=   8.5s\n",
      "[CV] splitter=best, min_samples_split=59, min_samples_leaf=16, max_depth=45, criterion=gini \n",
      "[CV]  splitter=best, min_samples_split=59, min_samples_leaf=16, max_depth=45, criterion=gini, total=   6.8s\n",
      "[CV] splitter=best, min_samples_split=59, min_samples_leaf=16, max_depth=45, criterion=gini \n",
      "[CV]  splitter=best, min_samples_split=59, min_samples_leaf=16, max_depth=45, criterion=gini, total=   7.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  5.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'splitter': 'best',\n",
       " 'min_samples_split': 59,\n",
       " 'min_samples_leaf': 16,\n",
       " 'max_depth': 45,\n",
       " 'criterion': 'gini'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier()\n",
    "possible_parameter_values = {'criterion' : ['gini', 'entropy'],\n",
    "                             'splitter' : ['best', 'random'],\n",
    "                             'max_depth' : [int(x) for x in np.arange(start = 5, stop = 101, step = 5)],\n",
    "                             'min_samples_split' : [int(x) for x in np.arange(start = 5, stop = 101, step = 1)],\n",
    "                             'min_samples_leaf' : [int(x) for x in np.arange(start = 1, stop = 51, step = 1)]}\n",
    "dtc_rscv = RandomizedSearchCV(estimator = dtc, param_distributions = possible_parameter_values, cv = 10, scoring = 'f1', verbose = 2, random_state = 17, n_jobs = 1)\n",
    "dtc_rscv.fit(X_train, y_train)\n",
    "dtc_rscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T12:49:14.745604Z",
     "start_time": "2021-04-18T12:49:14.074511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Train f1_score:  0.8253968253968255\n",
      "Validation f1_score:  0.7763157894736841\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Train precision:  0.9285714285714286\n",
      "Validation precision:  0.8082191780821918\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Train recall:  0.7428571428571429\n",
      "Validation recall:  0.7468354430379747\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x2dca97de5b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train_pred_dtc = dtc_rscv.predict(X_train)\n",
    "y_val_pred_dtc = dtc_rscv.predict(X_val)\n",
    "print('Decision Tree Classifier:')\n",
    "print('-' * 100)\n",
    "print('Train f1_score: ', f1_score(y_train, y_train_pred_dtc))\n",
    "print('Validation f1_score: ', f1_score(y_val, y_val_pred_dtc))\n",
    "print('-' * 100)\n",
    "print('Train precision: ', precision_score(y_train, y_train_pred_dtc))\n",
    "print('Validation precision: ', precision_score(y_val, y_val_pred_dtc))\n",
    "print('-' * 100)\n",
    "print('Train recall: ', recall_score(y_train, y_train_pred_dtc))\n",
    "print('Validation recall: ', recall_score(y_val, y_val_pred_dtc))\n",
    "print('-' * 100)\n",
    "plot_confusion_matrix(dtc_rscv, X_val, y_val)\n",
    "plot_roc_curve(dtc_rscv, X_val, y_val)\n",
    "plot_precision_recall_curve(dtc_rscv, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T12:57:06.227770Z",
     "start_time": "2021-04-18T12:57:06.220062Z"
    }
   },
   "outputs": [],
   "source": [
    "#rfc = RandomForestClassifier()\n",
    "#possible_parameter_values = {'n_estimators' : [int(x) for x in np.arange(start = 50, stop = 1001, step = 50)],\n",
    "#                             'criterion' : ['gini', 'entropy'],\n",
    "#                             'max_depth' : [int(x) for x in np.arange(start = 5, stop = 101, step = 5)],\n",
    "#                             'min_samples_split' : [int(x) for x in np.arange(start = 5, stop = 101, step = 1)],\n",
    "#                             'min_samples_leaf' : [int(x) for x in np.arange(start = 1, stop = 51, step = 1)]}\n",
    "#rfc_rscv = RandomizedSearchCV(estimator = rfc, param_distributions = possible_parameter_values, cv = 10, scoring = 'f1', verbose = 2, random_state = 17, n_jobs = 1)\n",
    "#rfc_rscv.fit(X_train, y_train)\n",
    "#rfc_rscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T13:00:52.721027Z",
     "start_time": "2021-04-18T13:00:52.714482Z"
    }
   },
   "outputs": [],
   "source": [
    "#y_train_pred_rfc = rfc_rscv.predict(X_train)\n",
    "#y_val_pred_rfc = rfc_rscv.predict(X_val)\n",
    "#print('Random Forest Classifier:')\n",
    "#print('-' * 100)\n",
    "#print('Train f1_score: ', f1_score(y_train, y_train_pred_rfc))\n",
    "#print('Validation f1_score: ', f1_score(y_val, y_val_pred_rfc))\n",
    "#print('-' * 100)\n",
    "#print('Train precision: ', precision_score(y_train, y_train_pred_rfc))\n",
    "#print('Validation precision: ', precision_score(y_val, y_val_pred_rfc))\n",
    "#print('-' * 100)\n",
    "#print('Train recall: ', recall_score(y_train, y_train_pred_rfc))\n",
    "#print('Validation recall: ', recall_score(y_val, y_val_pred_rfc))\n",
    "#print('-' * 100)\n",
    "#plot_confusion_matrix(rfc_rscv, X_val, y_val)\n",
    "#plot_roc_curve(rfc_rscv, X_val, y_val)\n",
    "#plot_precision_recall_curve(rfc_rscv, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:18:59.777281Z",
     "start_time": "2021-04-18T15:18:59.773323Z"
    }
   },
   "outputs": [],
   "source": [
    "#abc = AdaBoostClassifier()\n",
    "#possible_parameter_values = {'n_estimators' : [int(x) for x in np.arange(start = 50, stop = 1001, step = 50)],\n",
    "#                             'learning_rate' : [float(x)/1000 for x in np.arange(start = 1, stop = 1001, step = 1)]}\n",
    "#abc_rscv = RandomizedSearchCV(estimator = abc, param_distributions = possible_parameter_values, cv = 10, scoring = 'f1', verbose = 2, random_state = 17, n_jobs = 1)\n",
    "#abc_rscv.fit(X_train, y_train)\n",
    "#abc_rscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:21:06.325489Z",
     "start_time": "2021-04-18T15:21:06.322492Z"
    }
   },
   "outputs": [],
   "source": [
    "#y_train_pred_abc = abc_rscv.predict(X_train)\n",
    "#y_val_pred_abc = abc_rscv.predict(X_val)\n",
    "#print('Random Forest Classifier:')\n",
    "#print('-' * 100)\n",
    "#print('Train f1_score: ', f1_score(y_train, y_train_pred_abc))\n",
    "#print('Validation f1_score: ', f1_score(y_val, y_val_pred_abc))\n",
    "#print('-' * 100)\n",
    "#print('Train precision: ', precision_score(y_train, y_train_pred_abc))\n",
    "#print('Validation precision: ', precision_score(y_val, y_val_pred_abc))\n",
    "#print('-' * 100)\n",
    "#print('Train recall: ', recall_score(y_train, y_train_pred_abc))\n",
    "#print('Validation recall: ', recall_score(y_val, y_val_pred_abc))\n",
    "#print('-' * 100)\n",
    "#plot_confusion_matrix(abc_rscv, X_val, y_val)\n",
    "#plot_roc_curve(abc_rscv, X_val, y_val)\n",
    "#plot_precision_recall_curve(abc_rscv, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:29:09.656133Z",
     "start_time": "2021-04-18T15:29:09.652169Z"
    }
   },
   "outputs": [],
   "source": [
    "#gbc = GradientBoostingClassifier()\n",
    "#possible_parameter_values = {'learning_rate' : [float(x)/1000 for x in np.arange(start = 1, stop = 1001, step = 1)],\n",
    "#                             'n_estimators' : [int(x) for x in np.arange(start = 50, stop = 1001, step = 50)],\n",
    "#                             'max_depth' : [int(x) for x in np.arange(start = 5, stop = 101, step = 5)],\n",
    "#                             'min_samples_split' : [int(x) for x in np.arange(start = 5, stop = 101, step = 1)],\n",
    "#                             'min_samples_leaf' : [int(x) for x in np.arange(start = 1, stop = 51, step = 1)]}\n",
    "#gbc_rscv = RandomizedSearchCV(estimator = gbc, param_distributions = possible_parameter_values, cv = 10, scoring = 'f1', verbose = 2, random_state = 17, n_jobs = 1)\n",
    "#gbc_rscv.fit(X_train, y_train)\n",
    "#gbc_rscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T15:33:06.493469Z",
     "start_time": "2021-04-18T15:33:06.478247Z"
    }
   },
   "outputs": [],
   "source": [
    "#y_train_pred_gbc = gbc_rscv.predict(X_train)\n",
    "#y_val_pred_gbc = gbc_rscv.predict(X_val)\n",
    "#print('Random Forest Classifier:')\n",
    "#print('-' * 100)\n",
    "#print('Train f1_score: ', f1_score(y_train, y_train_pred_gbc))\n",
    "#print('Validation f1_score: ', f1_score(y_val, y_val_pred_gbc))\n",
    "#print('-' * 100)\n",
    "#print('Train precision: ', precision_score(y_train, y_train_pred_gbc))\n",
    "#print('Validation precision: ', precision_score(y_val, y_val_pred_gbc))\n",
    "#print('-' * 100)\n",
    "#print('Train recall: ', recall_score(y_train, y_train_pred_gbc))\n",
    "#print('Validation recall: ', recall_score(y_val, y_val_pred_gbc))\n",
    "#print('-' * 100)\n",
    "#plot_confusion_matrix(gbc_rscv, X_val, y_val)\n",
    "#plot_roc_curve(gbc_rscv, X_val, y_val)\n",
    "#plot_precision_recall_curve(gbc_rscv, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T16:31:39.456647Z",
     "start_time": "2021-04-18T15:38:57.965948Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[CV] use_label_encoder=False, n_estimators=50, max_depth=35, learning_rate=0.737 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:08:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=50, max_depth=35, learning_rate=0.737, total=   8.4s\n",
      "[CV] use_label_encoder=False, n_estimators=50, max_depth=35, learning_rate=0.737 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:09:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=50, max_depth=35, learning_rate=0.737, total=   7.4s\n",
      "[CV] use_label_encoder=False, n_estimators=50, max_depth=35, learning_rate=0.737 \n",
      "[21:09:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=50, max_depth=35, learning_rate=0.737, total=   7.6s\n",
      "[CV] use_label_encoder=False, n_estimators=50, max_depth=35, learning_rate=0.737 \n",
      "[21:09:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=50, max_depth=35, learning_rate=0.737, total=   7.7s\n",
      "[CV] use_label_encoder=False, n_estimators=50, max_depth=35, learning_rate=0.737 \n",
      "[21:09:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=50, max_depth=35, learning_rate=0.737, total=   7.7s\n",
      "[CV] use_label_encoder=False, n_estimators=50, max_depth=35, learning_rate=0.737 \n",
      "[21:09:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=50, max_depth=35, learning_rate=0.737, total=   7.6s\n",
      "[CV] use_label_encoder=False, n_estimators=50, max_depth=35, learning_rate=0.737 \n",
      "[21:09:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=50, max_depth=35, learning_rate=0.737, total=   7.5s\n",
      "[CV] use_label_encoder=False, n_estimators=50, max_depth=35, learning_rate=0.737 \n",
      "[21:09:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=50, max_depth=35, learning_rate=0.737, total=   7.8s\n",
      "[CV] use_label_encoder=False, n_estimators=50, max_depth=35, learning_rate=0.737 \n",
      "[21:10:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=50, max_depth=35, learning_rate=0.737, total=   7.8s\n",
      "[CV] use_label_encoder=False, n_estimators=50, max_depth=35, learning_rate=0.737 \n",
      "[21:10:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=50, max_depth=35, learning_rate=0.737, total=   7.7s\n",
      "[CV] use_label_encoder=False, n_estimators=600, max_depth=40, learning_rate=0.372 \n",
      "[21:10:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=600, max_depth=40, learning_rate=0.372, total=  38.1s\n",
      "[CV] use_label_encoder=False, n_estimators=600, max_depth=40, learning_rate=0.372 \n",
      "[21:10:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=600, max_depth=40, learning_rate=0.372, total=  37.9s\n",
      "[CV] use_label_encoder=False, n_estimators=600, max_depth=40, learning_rate=0.372 \n",
      "[21:11:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=600, max_depth=40, learning_rate=0.372, total=  39.5s\n",
      "[CV] use_label_encoder=False, n_estimators=600, max_depth=40, learning_rate=0.372 \n",
      "[21:12:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=600, max_depth=40, learning_rate=0.372, total=  38.0s\n",
      "[CV] use_label_encoder=False, n_estimators=600, max_depth=40, learning_rate=0.372 \n",
      "[21:12:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=600, max_depth=40, learning_rate=0.372, total=  37.8s\n",
      "[CV] use_label_encoder=False, n_estimators=600, max_depth=40, learning_rate=0.372 \n",
      "[21:13:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=600, max_depth=40, learning_rate=0.372, total=  37.1s\n",
      "[CV] use_label_encoder=False, n_estimators=600, max_depth=40, learning_rate=0.372 \n",
      "[21:14:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=600, max_depth=40, learning_rate=0.372, total=  39.9s\n",
      "[CV] use_label_encoder=False, n_estimators=600, max_depth=40, learning_rate=0.372 \n",
      "[21:14:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=600, max_depth=40, learning_rate=0.372, total=  38.6s\n",
      "[CV] use_label_encoder=False, n_estimators=600, max_depth=40, learning_rate=0.372 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:15:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=600, max_depth=40, learning_rate=0.372, total=  39.1s\n",
      "[CV] use_label_encoder=False, n_estimators=600, max_depth=40, learning_rate=0.372 \n",
      "[21:16:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=600, max_depth=40, learning_rate=0.372, total=  36.7s\n",
      "[CV] use_label_encoder=True, n_estimators=200, max_depth=95, learning_rate=0.615 \n",
      "[21:16:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=200, max_depth=95, learning_rate=0.615, total=  17.8s\n",
      "[CV] use_label_encoder=True, n_estimators=200, max_depth=95, learning_rate=0.615 \n",
      "[21:16:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=200, max_depth=95, learning_rate=0.615, total=  17.1s\n",
      "[CV] use_label_encoder=True, n_estimators=200, max_depth=95, learning_rate=0.615 \n",
      "[21:17:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=200, max_depth=95, learning_rate=0.615, total=  17.9s\n",
      "[CV] use_label_encoder=True, n_estimators=200, max_depth=95, learning_rate=0.615 \n",
      "[21:17:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=200, max_depth=95, learning_rate=0.615, total=  17.4s\n",
      "[CV] use_label_encoder=True, n_estimators=200, max_depth=95, learning_rate=0.615 \n",
      "[21:17:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=200, max_depth=95, learning_rate=0.615, total=  17.1s\n",
      "[CV] use_label_encoder=True, n_estimators=200, max_depth=95, learning_rate=0.615 \n",
      "[21:18:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=200, max_depth=95, learning_rate=0.615, total=  16.6s\n",
      "[CV] use_label_encoder=True, n_estimators=200, max_depth=95, learning_rate=0.615 \n",
      "[21:18:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=200, max_depth=95, learning_rate=0.615, total=  17.0s\n",
      "[CV] use_label_encoder=True, n_estimators=200, max_depth=95, learning_rate=0.615 \n",
      "[21:18:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=200, max_depth=95, learning_rate=0.615, total=  17.2s\n",
      "[CV] use_label_encoder=True, n_estimators=200, max_depth=95, learning_rate=0.615 \n",
      "[21:18:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=200, max_depth=95, learning_rate=0.615, total=  17.3s\n",
      "[CV] use_label_encoder=True, n_estimators=200, max_depth=95, learning_rate=0.615 \n",
      "[21:19:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=200, max_depth=95, learning_rate=0.615, total=  16.7s\n",
      "[CV] use_label_encoder=True, n_estimators=50, max_depth=15, learning_rate=0.158 \n",
      "[21:19:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=50, max_depth=15, learning_rate=0.158, total=  12.9s\n",
      "[CV] use_label_encoder=True, n_estimators=50, max_depth=15, learning_rate=0.158 \n",
      "[21:19:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=50, max_depth=15, learning_rate=0.158, total=  12.8s\n",
      "[CV] use_label_encoder=True, n_estimators=50, max_depth=15, learning_rate=0.158 \n",
      "[21:19:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=50, max_depth=15, learning_rate=0.158, total=  12.7s\n",
      "[CV] use_label_encoder=True, n_estimators=50, max_depth=15, learning_rate=0.158 \n",
      "[21:20:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=50, max_depth=15, learning_rate=0.158, total=  12.5s\n",
      "[CV] use_label_encoder=True, n_estimators=50, max_depth=15, learning_rate=0.158 \n",
      "[21:20:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=50, max_depth=15, learning_rate=0.158, total=  12.7s\n",
      "[CV] use_label_encoder=True, n_estimators=50, max_depth=15, learning_rate=0.158 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:20:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=50, max_depth=15, learning_rate=0.158, total=  12.5s\n",
      "[CV] use_label_encoder=True, n_estimators=50, max_depth=15, learning_rate=0.158 \n",
      "[21:20:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=50, max_depth=15, learning_rate=0.158, total=  13.5s\n",
      "[CV] use_label_encoder=True, n_estimators=50, max_depth=15, learning_rate=0.158 \n",
      "[21:21:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=50, max_depth=15, learning_rate=0.158, total=  12.8s\n",
      "[CV] use_label_encoder=True, n_estimators=50, max_depth=15, learning_rate=0.158 \n",
      "[21:21:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=50, max_depth=15, learning_rate=0.158, total=  12.5s\n",
      "[CV] use_label_encoder=True, n_estimators=50, max_depth=15, learning_rate=0.158 \n",
      "[21:21:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=50, max_depth=15, learning_rate=0.158, total=  12.1s\n",
      "[CV] use_label_encoder=False, n_estimators=750, max_depth=15, learning_rate=0.971 \n",
      "[21:21:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=750, max_depth=15, learning_rate=0.971, total=  28.1s\n",
      "[CV] use_label_encoder=False, n_estimators=750, max_depth=15, learning_rate=0.971 \n",
      "[21:22:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=750, max_depth=15, learning_rate=0.971, total=  28.2s\n",
      "[CV] use_label_encoder=False, n_estimators=750, max_depth=15, learning_rate=0.971 \n",
      "[21:22:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=750, max_depth=15, learning_rate=0.971, total=  29.5s\n",
      "[CV] use_label_encoder=False, n_estimators=750, max_depth=15, learning_rate=0.971 \n",
      "[21:23:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=750, max_depth=15, learning_rate=0.971, total=  28.5s\n",
      "[CV] use_label_encoder=False, n_estimators=750, max_depth=15, learning_rate=0.971 \n",
      "[21:23:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=750, max_depth=15, learning_rate=0.971, total=  28.1s\n",
      "[CV] use_label_encoder=False, n_estimators=750, max_depth=15, learning_rate=0.971 \n",
      "[21:24:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=750, max_depth=15, learning_rate=0.971, total=  27.2s\n",
      "[CV] use_label_encoder=False, n_estimators=750, max_depth=15, learning_rate=0.971 \n",
      "[21:24:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=750, max_depth=15, learning_rate=0.971, total=  28.1s\n",
      "[CV] use_label_encoder=False, n_estimators=750, max_depth=15, learning_rate=0.971 \n",
      "[21:24:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=750, max_depth=15, learning_rate=0.971, total=  27.8s\n",
      "[CV] use_label_encoder=False, n_estimators=750, max_depth=15, learning_rate=0.971 \n",
      "[21:25:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=750, max_depth=15, learning_rate=0.971, total=  28.8s\n",
      "[CV] use_label_encoder=False, n_estimators=750, max_depth=15, learning_rate=0.971 \n",
      "[21:25:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=750, max_depth=15, learning_rate=0.971, total=  27.7s\n",
      "[CV] use_label_encoder=True, n_estimators=400, max_depth=50, learning_rate=0.646 \n",
      "[21:26:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=400, max_depth=50, learning_rate=0.646, total=  23.7s\n",
      "[CV] use_label_encoder=True, n_estimators=400, max_depth=50, learning_rate=0.646 \n",
      "[21:26:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=400, max_depth=50, learning_rate=0.646, total=  22.8s\n",
      "[CV] use_label_encoder=True, n_estimators=400, max_depth=50, learning_rate=0.646 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:27:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=400, max_depth=50, learning_rate=0.646, total=  24.4s\n",
      "[CV] use_label_encoder=True, n_estimators=400, max_depth=50, learning_rate=0.646 \n",
      "[21:27:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=400, max_depth=50, learning_rate=0.646, total=  22.9s\n",
      "[CV] use_label_encoder=True, n_estimators=400, max_depth=50, learning_rate=0.646 \n",
      "[21:27:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=400, max_depth=50, learning_rate=0.646, total=  22.9s\n",
      "[CV] use_label_encoder=True, n_estimators=400, max_depth=50, learning_rate=0.646 \n",
      "[21:28:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=400, max_depth=50, learning_rate=0.646, total=  22.1s\n",
      "[CV] use_label_encoder=True, n_estimators=400, max_depth=50, learning_rate=0.646 \n",
      "[21:28:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=400, max_depth=50, learning_rate=0.646, total=  23.1s\n",
      "[CV] use_label_encoder=True, n_estimators=400, max_depth=50, learning_rate=0.646 \n",
      "[21:29:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=400, max_depth=50, learning_rate=0.646, total=  23.4s\n",
      "[CV] use_label_encoder=True, n_estimators=400, max_depth=50, learning_rate=0.646 \n",
      "[21:29:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=400, max_depth=50, learning_rate=0.646, total=  23.6s\n",
      "[CV] use_label_encoder=True, n_estimators=400, max_depth=50, learning_rate=0.646 \n",
      "[21:29:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=400, max_depth=50, learning_rate=0.646, total=  22.4s\n",
      "[CV] use_label_encoder=False, n_estimators=400, max_depth=75, learning_rate=0.36 \n",
      "[21:30:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=400, max_depth=75, learning_rate=0.36, total=  31.8s\n",
      "[CV] use_label_encoder=False, n_estimators=400, max_depth=75, learning_rate=0.36 \n",
      "[21:30:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=400, max_depth=75, learning_rate=0.36, total=  31.4s\n",
      "[CV] use_label_encoder=False, n_estimators=400, max_depth=75, learning_rate=0.36 \n",
      "[21:31:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=400, max_depth=75, learning_rate=0.36, total=  33.2s\n",
      "[CV] use_label_encoder=False, n_estimators=400, max_depth=75, learning_rate=0.36 \n",
      "[21:31:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=400, max_depth=75, learning_rate=0.36, total=  31.7s\n",
      "[CV] use_label_encoder=False, n_estimators=400, max_depth=75, learning_rate=0.36 \n",
      "[21:32:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=400, max_depth=75, learning_rate=0.36, total=  31.9s\n",
      "[CV] use_label_encoder=False, n_estimators=400, max_depth=75, learning_rate=0.36 \n",
      "[21:32:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=400, max_depth=75, learning_rate=0.36, total=  30.4s\n",
      "[CV] use_label_encoder=False, n_estimators=400, max_depth=75, learning_rate=0.36 \n",
      "[21:33:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=400, max_depth=75, learning_rate=0.36, total=  31.6s\n",
      "[CV] use_label_encoder=False, n_estimators=400, max_depth=75, learning_rate=0.36 \n",
      "[21:33:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=400, max_depth=75, learning_rate=0.36, total=  30.8s\n",
      "[CV] use_label_encoder=False, n_estimators=400, max_depth=75, learning_rate=0.36 \n",
      "[21:34:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=400, max_depth=75, learning_rate=0.36, total=  32.1s\n",
      "[CV] use_label_encoder=False, n_estimators=400, max_depth=75, learning_rate=0.36 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:34:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=False, n_estimators=400, max_depth=75, learning_rate=0.36, total=  30.4s\n",
      "[CV] use_label_encoder=True, n_estimators=150, max_depth=45, learning_rate=0.449 \n",
      "[21:35:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=150, max_depth=45, learning_rate=0.449, total=  17.3s\n",
      "[CV] use_label_encoder=True, n_estimators=150, max_depth=45, learning_rate=0.449 \n",
      "[21:35:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=150, max_depth=45, learning_rate=0.449, total=  17.3s\n",
      "[CV] use_label_encoder=True, n_estimators=150, max_depth=45, learning_rate=0.449 \n",
      "[21:36:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=150, max_depth=45, learning_rate=0.449, total=  18.3s\n",
      "[CV] use_label_encoder=True, n_estimators=150, max_depth=45, learning_rate=0.449 \n",
      "[21:36:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=150, max_depth=45, learning_rate=0.449, total=  17.4s\n",
      "[CV] use_label_encoder=True, n_estimators=150, max_depth=45, learning_rate=0.449 \n",
      "[21:36:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=150, max_depth=45, learning_rate=0.449, total=  17.2s\n",
      "[CV] use_label_encoder=True, n_estimators=150, max_depth=45, learning_rate=0.449 \n",
      "[21:36:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=150, max_depth=45, learning_rate=0.449, total=  16.8s\n",
      "[CV] use_label_encoder=True, n_estimators=150, max_depth=45, learning_rate=0.449 \n",
      "[21:37:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=150, max_depth=45, learning_rate=0.449, total=  17.7s\n",
      "[CV] use_label_encoder=True, n_estimators=150, max_depth=45, learning_rate=0.449 \n",
      "[21:37:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=150, max_depth=45, learning_rate=0.449, total=  17.1s\n",
      "[CV] use_label_encoder=True, n_estimators=150, max_depth=45, learning_rate=0.449 \n",
      "[21:37:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=150, max_depth=45, learning_rate=0.449, total=  17.4s\n",
      "[CV] use_label_encoder=True, n_estimators=150, max_depth=45, learning_rate=0.449 \n",
      "[21:38:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=150, max_depth=45, learning_rate=0.449, total=  16.8s\n",
      "[CV] use_label_encoder=True, n_estimators=950, max_depth=35, learning_rate=0.375 \n",
      "[21:38:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=950, max_depth=35, learning_rate=0.375, total=  48.6s\n",
      "[CV] use_label_encoder=True, n_estimators=950, max_depth=35, learning_rate=0.375 \n",
      "[21:39:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=950, max_depth=35, learning_rate=0.375, total=  46.8s\n",
      "[CV] use_label_encoder=True, n_estimators=950, max_depth=35, learning_rate=0.375 \n",
      "[21:39:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=950, max_depth=35, learning_rate=0.375, total=  50.0s\n",
      "[CV] use_label_encoder=True, n_estimators=950, max_depth=35, learning_rate=0.375 \n",
      "[21:40:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=950, max_depth=35, learning_rate=0.375, total=  48.2s\n",
      "[CV] use_label_encoder=True, n_estimators=950, max_depth=35, learning_rate=0.375 \n",
      "[21:41:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=950, max_depth=35, learning_rate=0.375, total=  47.4s\n",
      "[CV] use_label_encoder=True, n_estimators=950, max_depth=35, learning_rate=0.375 \n",
      "[21:42:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=950, max_depth=35, learning_rate=0.375, total=  46.3s\n",
      "[CV] use_label_encoder=True, n_estimators=950, max_depth=35, learning_rate=0.375 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:43:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=950, max_depth=35, learning_rate=0.375, total=  47.9s\n",
      "[CV] use_label_encoder=True, n_estimators=950, max_depth=35, learning_rate=0.375 \n",
      "[21:43:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=950, max_depth=35, learning_rate=0.375, total=  46.9s\n",
      "[CV] use_label_encoder=True, n_estimators=950, max_depth=35, learning_rate=0.375 \n",
      "[21:44:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=950, max_depth=35, learning_rate=0.375, total=  48.4s\n",
      "[CV] use_label_encoder=True, n_estimators=950, max_depth=35, learning_rate=0.375 \n",
      "[21:45:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=950, max_depth=35, learning_rate=0.375, total=  46.3s\n",
      "[CV] use_label_encoder=True, n_estimators=750, max_depth=75, learning_rate=0.1 \n",
      "[21:46:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=750, max_depth=75, learning_rate=0.1, total= 1.4min\n",
      "[CV] use_label_encoder=True, n_estimators=750, max_depth=75, learning_rate=0.1 \n",
      "[21:47:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=750, max_depth=75, learning_rate=0.1, total= 1.4min\n",
      "[CV] use_label_encoder=True, n_estimators=750, max_depth=75, learning_rate=0.1 \n",
      "[21:49:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=750, max_depth=75, learning_rate=0.1, total= 1.4min\n",
      "[CV] use_label_encoder=True, n_estimators=750, max_depth=75, learning_rate=0.1 \n",
      "[21:50:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=750, max_depth=75, learning_rate=0.1, total= 1.4min\n",
      "[CV] use_label_encoder=True, n_estimators=750, max_depth=75, learning_rate=0.1 \n",
      "[21:51:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=750, max_depth=75, learning_rate=0.1, total= 1.4min\n",
      "[CV] use_label_encoder=True, n_estimators=750, max_depth=75, learning_rate=0.1 \n",
      "[21:53:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=750, max_depth=75, learning_rate=0.1, total= 1.3min\n",
      "[CV] use_label_encoder=True, n_estimators=750, max_depth=75, learning_rate=0.1 \n",
      "[21:54:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=750, max_depth=75, learning_rate=0.1, total= 1.4min\n",
      "[CV] use_label_encoder=True, n_estimators=750, max_depth=75, learning_rate=0.1 \n",
      "[21:55:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=750, max_depth=75, learning_rate=0.1, total= 1.4min\n",
      "[CV] use_label_encoder=True, n_estimators=750, max_depth=75, learning_rate=0.1 \n",
      "[21:57:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=750, max_depth=75, learning_rate=0.1, total= 1.4min\n",
      "[CV] use_label_encoder=True, n_estimators=750, max_depth=75, learning_rate=0.1 \n",
      "[21:58:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  use_label_encoder=True, n_estimators=750, max_depth=75, learning_rate=0.1, total= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed: 51.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:00:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'use_label_encoder': True,\n",
       " 'n_estimators': 750,\n",
       " 'max_depth': 75,\n",
       " 'learning_rate': 0.1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc = XGBClassifier()\n",
    "possible_parameter_values = {'n_estimators' : [int(x) for x in np.arange(start = 50, stop = 1001, step = 50)], \n",
    "                             'use_label_encoder' : [True, False], \n",
    "                             'max_depth' : [int(x) for x in np.arange(start = 5, stop = 101, step = 5)], \n",
    "                             'learning_rate' : [float(x)/1000 for x in np.arange(start = 1, stop = 1001, step = 1)]}\n",
    "xgbc_rscv = RandomizedSearchCV(estimator = xgbc, param_distributions = possible_parameter_values, cv = 10, scoring = 'f1', verbose = 2, random_state = 17, n_jobs = 1)\n",
    "xgbc_rscv.fit(X_train, y_train)\n",
    "xgbc_rscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T16:32:38.167441Z",
     "start_time": "2021-04-18T16:32:36.334956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Train f1_score:  1.0\n",
      "Validation f1_score:  0.87248322147651\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Train precision:  1.0\n",
      "Validation precision:  0.9285714285714286\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Train recall:  1.0\n",
      "Validation recall:  0.8227848101265823\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x176d34233a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train_pred_xgbc = xgbc_rscv.predict(X_train)\n",
    "y_val_pred_xgbc = xgbc_rscv.predict(X_val)\n",
    "print('XGBoost Classifier:')\n",
    "print('-' * 100)\n",
    "print('Train f1_score: ', f1_score(y_train, y_train_pred_xgbc))\n",
    "print('Validation f1_score: ', f1_score(y_val, y_val_pred_xgbc))\n",
    "print('-' * 100)\n",
    "print('Train precision: ', precision_score(y_train, y_train_pred_xgbc))\n",
    "print('Validation precision: ', precision_score(y_val, y_val_pred_xgbc))\n",
    "print('-' * 100)\n",
    "print('Train recall: ', recall_score(y_train, y_train_pred_xgbc))\n",
    "print('Validation recall: ', recall_score(y_val, y_val_pred_xgbc))\n",
    "print('-' * 100)\n",
    "plot_confusion_matrix(xgbc_rscv, X_val, y_val)\n",
    "plot_roc_curve(xgbc_rscv, X_val, y_val)\n",
    "plot_precision_recall_curve(xgbc_rscv, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T16:47:16.383328Z",
     "start_time": "2021-04-18T16:47:16.370853Z"
    }
   },
   "outputs": [],
   "source": [
    "#knc = KNeighborsClassifier()\n",
    "#possible_parameter_values = {'n_neighbors' : [int(x) for x in np.arange(start = 1, stop = 101, step = 1)],\n",
    "#                             'weights' : ['uniform', 'distance']}\n",
    "#knc_rscv = RandomizedSearchCV(estimator = knc, param_distributions = possible_parameter_values, cv = 10, scoring = 'f1', verbose = 2, random_state = 17, n_jobs = 1)\n",
    "#knc_rscv.fit(X_train, y_train)\n",
    "#knc_rscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T16:48:20.523997Z",
     "start_time": "2021-04-18T16:48:20.516019Z"
    }
   },
   "outputs": [],
   "source": [
    "#y_train_pred_knc = knc_rscv.predict(X_train)\n",
    "#y_val_pred_knc = knc_rscv.predict(X_val)\n",
    "#print('XGBoost Classifier:')\n",
    "#print('-' * 100)\n",
    "#print('Train f1_score: ', f1_score(y_train, y_train_pred_knc))\n",
    "#print('Validation f1_score: ', f1_score(y_val, y_val_pred_knc))\n",
    "#print('-' * 100)\n",
    "#print('Train precision: ', precision_score(y_train, y_train_pred_knc))\n",
    "#print('Validation precision: ', precision_score(y_val, y_val_pred_knc))\n",
    "#print('-' * 100)\n",
    "#print('Train recall: ', recall_score(y_train, y_train_pred_knc))\n",
    "#print('Validation recall: ', recall_score(y_val, y_val_pred_knc))\n",
    "#print('-' * 100)\n",
    "#plot_confusion_matrix(knc_rscv, X_val, y_val)\n",
    "#plot_roc_curve(knc_rscv, X_val, y_val)\n",
    "#plot_precision_recall_curve(knc_rscv, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T17:53:14.366343Z",
     "start_time": "2021-04-18T17:53:14.358225Z"
    }
   },
   "outputs": [],
   "source": [
    "#svc = SVC()\n",
    "#possible_parameter_values = {'C' : [float(x)/10 for x in np.arange(start = 1, stop = 1001, step = 1)],\n",
    "#                             'gamma' : [float(x)/10000 for x in np.arange(start = 1, stop = 100000, step = 1)]}\n",
    "#svc_rscv = RandomizedSearchCV(estimator = svc, param_distributions = possible_parameter_values, cv = 10, scoring = 'f1', verbose = 2, random_state = 17, n_jobs = 1)\n",
    "#svc_rscv.fit(X_train, y_train)\n",
    "#svc_rscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T17:53:44.699362Z",
     "start_time": "2021-04-18T17:53:44.694555Z"
    }
   },
   "outputs": [],
   "source": [
    "#y_train_pred_svc = svc_rscv.predict(X_train)\n",
    "#y_val_pred_svc = svc_rscv.predict(X_val)\n",
    "#print('XGBoost Classifier:')\n",
    "#print('-' * 100)\n",
    "#print('Train f1_score: ', f1_score(y_train, y_train_pred_svc))\n",
    "#print('Validation f1_score: ', f1_score(y_val, y_val_pred_svc))\n",
    "#print('-' * 100)\n",
    "#print('Train precision: ', precision_score(y_train, y_train_pred_svc))\n",
    "#print('Validation precision: ', precision_score(y_val, y_val_pred_svc))\n",
    "#print('-' * 100)\n",
    "#print('Train recall: ', recall_score(y_train, y_train_pred_svc))\n",
    "#print('Validation recall: ', recall_score(y_val, y_val_pred_svc))\n",
    "#print('-' * 100)\n",
    "#plot_confusion_matrix(svc_rscv, X_val, y_val)\n",
    "#plot_roc_curve(svc_rscv, X_val, y_val)\n",
    "#plot_precision_recall_curve(svc_rscv, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:25:46.007928Z",
     "start_time": "2021-04-18T18:25:27.305097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Cross Validation:\n",
      "\n",
      "Mean Difference(f1):  4.660510756558949\n",
      "Mean Difference(Precision):  6.430511314794375\n",
      "Mean Difference(Recall):  6.581258087255546\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Random State</th>\n",
       "      <th>Train f1</th>\n",
       "      <th>Validation f1</th>\n",
       "      <th>Train Precision</th>\n",
       "      <th>Validation Precision</th>\n",
       "      <th>Train Recall</th>\n",
       "      <th>Validation Recall</th>\n",
       "      <th>Difference(f1)</th>\n",
       "      <th>Difference(Precision)</th>\n",
       "      <th>Difference(Recall)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>0.752727</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.879310</td>\n",
       "      <td>0.655063</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.362319</td>\n",
       "      <td>0.599700</td>\n",
       "      <td>0.185805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.639344</td>\n",
       "      <td>0.662921</td>\n",
       "      <td>1.824458</td>\n",
       "      <td>0.650593</td>\n",
       "      <td>3.687698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>0.776735</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.907895</td>\n",
       "      <td>0.816901</td>\n",
       "      <td>0.678689</td>\n",
       "      <td>0.651685</td>\n",
       "      <td>6.660628</td>\n",
       "      <td>10.022454</td>\n",
       "      <td>3.978722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.885246</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>14.322917</td>\n",
       "      <td>12.139918</td>\n",
       "      <td>15.915916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80</td>\n",
       "      <td>0.742115</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>0.873362</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.559524</td>\n",
       "      <td>6.863971</td>\n",
       "      <td>3.490385</td>\n",
       "      <td>13.273810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>46</td>\n",
       "      <td>0.741874</td>\n",
       "      <td>0.703448</td>\n",
       "      <td>0.877828</td>\n",
       "      <td>0.962264</td>\n",
       "      <td>0.642384</td>\n",
       "      <td>0.554348</td>\n",
       "      <td>5.179524</td>\n",
       "      <td>9.618751</td>\n",
       "      <td>13.704617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13</td>\n",
       "      <td>0.784810</td>\n",
       "      <td>0.736111</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.670886</td>\n",
       "      <td>6.205197</td>\n",
       "      <td>10.570720</td>\n",
       "      <td>2.613312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.738574</td>\n",
       "      <td>0.748092</td>\n",
       "      <td>0.882096</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.635220</td>\n",
       "      <td>0.644737</td>\n",
       "      <td>1.288640</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>1.498176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "      <td>0.729242</td>\n",
       "      <td>0.713043</td>\n",
       "      <td>0.866953</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.629283</td>\n",
       "      <td>0.561644</td>\n",
       "      <td>2.221266</td>\n",
       "      <td>12.600189</td>\n",
       "      <td>10.748678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25</td>\n",
       "      <td>0.738404</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>0.872807</td>\n",
       "      <td>0.841270</td>\n",
       "      <td>0.639871</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>1.676189</td>\n",
       "      <td>3.613305</td>\n",
       "      <td>0.205849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Random State  Train f1  Validation f1  Train Precision  \\\n",
       "0            17  0.752727       0.750000         0.884615   \n",
       "1            15  0.742857       0.756410         0.886364   \n",
       "2            26  0.776735       0.725000         0.907895   \n",
       "3             1  0.765957       0.656250         0.885246   \n",
       "4            80  0.742115       0.691176         0.873362   \n",
       "5            46  0.741874       0.703448         0.877828   \n",
       "6            13  0.784810       0.736111         0.911765   \n",
       "7             0  0.738574       0.748092         0.882096   \n",
       "8            16  0.729242       0.713043         0.866953   \n",
       "9            25  0.738404       0.726027         0.872807   \n",
       "\n",
       "   Validation Precision  Train Recall  Validation Recall  Difference(f1)  \\\n",
       "0              0.879310      0.655063           0.653846        0.362319   \n",
       "1              0.880597      0.639344           0.662921        1.824458   \n",
       "2              0.816901      0.678689           0.651685        6.660628   \n",
       "3              0.777778      0.675000           0.567568       14.322917   \n",
       "4              0.903846      0.645161           0.559524        6.863971   \n",
       "5              0.962264      0.642384           0.554348        5.179524   \n",
       "6              0.815385      0.688889           0.670886        6.205197   \n",
       "7              0.890909      0.635220           0.644737        1.288640   \n",
       "8              0.976190      0.629283           0.561644        2.221266   \n",
       "9              0.841270      0.639871           0.638554        1.676189   \n",
       "\n",
       "   Difference(Precision)  Difference(Recall)  \n",
       "0               0.599700            0.185805  \n",
       "1               0.650593            3.687698  \n",
       "2              10.022454            3.978722  \n",
       "3              12.139918           15.915916  \n",
       "4               3.490385           13.273810  \n",
       "5               9.618751           13.704617  \n",
       "6              10.570720            2.613312  \n",
       "7               0.999100            1.498176  \n",
       "8              12.600189           10.748678  \n",
       "9               3.613305            0.205849  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Logistic Regression Cross Validation:\\n')\n",
    "f1_train = []\n",
    "f1_val = []\n",
    "precision_train = []\n",
    "precision_val = []\n",
    "recall_train = []\n",
    "recall_val = []\n",
    "random_state = []\n",
    "X = X_scaled\n",
    "y = df_train['Class']\n",
    "for i in range(0,10):\n",
    "    rs = random.randint(0, 100)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = rs)\n",
    "    test_model_lr = LogisticRegression()\n",
    "    test_model_lr.fit(X_train, y_train)\n",
    "    y_train_pred_tlr = test_model_lr.predict(X_train)\n",
    "    y_val_pred_tlr = test_model_lr.predict(X_val)\n",
    "    random_state.append(rs)\n",
    "    f1_train.append(f1_score(y_train, y_train_pred_tlr))\n",
    "    f1_val.append(f1_score(y_val, y_val_pred_tlr))\n",
    "    precision_train.append(precision_score(y_train, y_train_pred_tlr))\n",
    "    precision_val.append(precision_score(y_val, y_val_pred_tlr))\n",
    "    recall_train.append(recall_score(y_train, y_train_pred_tlr))\n",
    "    recall_val.append(recall_score(y_val, y_val_pred_tlr))\n",
    "data = {'Random State' : random_state, 'Train f1' : f1_train, 'Validation f1' : f1_val, 'Train Precision' : precision_train, 'Validation Precision' : precision_val, \n",
    "        'Train Recall' : recall_train, 'Validation Recall' : recall_val}\n",
    "data = pd.DataFrame(data)\n",
    "data['Difference(f1)'] = ((np.abs(data['Train f1'] - data['Validation f1'])) * 100)/(data['Train f1'])\n",
    "data['Difference(Precision)'] = ((np.abs(data['Train Precision'] - data['Validation Precision'])) * 100)/(data['Train Precision'])\n",
    "data['Difference(Recall)'] = ((np.abs(data['Train Recall'] - data['Validation Recall'])) * 100)/(data['Train Recall'])\n",
    "print('Mean Difference(f1): ', data['Difference(f1)'].mean())\n",
    "print('Mean Difference(Precision): ', data['Difference(Precision)'].mean())\n",
    "print('Mean Difference(Recall): ', data['Difference(Recall)'].mean())\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:29:40.531169Z",
     "start_time": "2021-04-18T18:28:07.996843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier Cross Validation:\n",
      "\n",
      "Mean Difference(f1):  5.719273226499161\n",
      "Mean Difference(Precision):  4.401483855214559\n",
      "Mean Difference(Recall):  8.420120068324907\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Random State</th>\n",
       "      <th>Train f1</th>\n",
       "      <th>Validation f1</th>\n",
       "      <th>Train Precision</th>\n",
       "      <th>Validation Precision</th>\n",
       "      <th>Train Recall</th>\n",
       "      <th>Validation Recall</th>\n",
       "      <th>Difference(f1)</th>\n",
       "      <th>Difference(Precision)</th>\n",
       "      <th>Difference(Recall)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.787402</td>\n",
       "      <td>0.913357</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.788162</td>\n",
       "      <td>0.684932</td>\n",
       "      <td>6.943450</td>\n",
       "      <td>1.376080</td>\n",
       "      <td>13.097623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "      <td>0.822898</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.744337</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>10.884058</td>\n",
       "      <td>8.026756</td>\n",
       "      <td>13.069054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>0.837790</td>\n",
       "      <td>0.723926</td>\n",
       "      <td>0.907336</td>\n",
       "      <td>0.830986</td>\n",
       "      <td>0.778146</td>\n",
       "      <td>0.641304</td>\n",
       "      <td>13.590915</td>\n",
       "      <td>8.414744</td>\n",
       "      <td>17.585569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57</td>\n",
       "      <td>0.808989</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>0.919149</td>\n",
       "      <td>0.905882</td>\n",
       "      <td>0.722408</td>\n",
       "      <td>0.810526</td>\n",
       "      <td>5.756173</td>\n",
       "      <td>1.443355</td>\n",
       "      <td>12.197856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>0.822107</td>\n",
       "      <td>0.827068</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.873016</td>\n",
       "      <td>0.734568</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.603399</td>\n",
       "      <td>6.462585</td>\n",
       "      <td>6.962785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>66</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.858896</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.958904</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>3.659827</td>\n",
       "      <td>5.810109</td>\n",
       "      <td>1.915709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>58</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.773585</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.533537</td>\n",
       "      <td>1.347882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>88</td>\n",
       "      <td>0.818636</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.866197</td>\n",
       "      <td>0.845070</td>\n",
       "      <td>0.776025</td>\n",
       "      <td>0.779221</td>\n",
       "      <td>0.955834</td>\n",
       "      <td>2.439024</td>\n",
       "      <td>0.411783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>91</td>\n",
       "      <td>0.816189</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.870504</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.768254</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>6.650925</td>\n",
       "      <td>5.396208</td>\n",
       "      <td>7.730934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>0.824818</td>\n",
       "      <td>0.883978</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.758389</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>7.172542</td>\n",
       "      <td>4.112441</td>\n",
       "      <td>9.882006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Random State  Train f1  Validation f1  Train Precision  \\\n",
       "0            16  0.846154       0.787402         0.913357   \n",
       "1            40  0.822898       0.733333         0.920000   \n",
       "2            46  0.837790       0.723926         0.907336   \n",
       "3            57  0.808989       0.855556         0.919149   \n",
       "4            34  0.822107       0.827068         0.933333   \n",
       "5            66  0.828571       0.858896         0.906250   \n",
       "6            58  0.836735       0.828571         0.911111   \n",
       "7            88  0.818636       0.810811         0.866197   \n",
       "8            91  0.816189       0.761905         0.870504   \n",
       "9             5  0.824818       0.883978         0.904000   \n",
       "\n",
       "   Validation Precision  Train Recall  Validation Recall  Difference(f1)  \\\n",
       "0              0.925926      0.788162           0.684932        6.943450   \n",
       "1              0.846154      0.744337           0.647059       10.884058   \n",
       "2              0.830986      0.778146           0.641304       13.590915   \n",
       "3              0.905882      0.722408           0.810526        5.756173   \n",
       "4              0.873016      0.734568           0.785714        0.603399   \n",
       "5              0.958904      0.763158           0.777778        3.659827   \n",
       "6              0.906250      0.773585           0.763158        0.975610   \n",
       "7              0.845070      0.776025           0.779221        0.955834   \n",
       "8              0.823529      0.768254           0.708861        6.650925   \n",
       "9              0.941176      0.758389           0.833333        7.172542   \n",
       "\n",
       "   Difference(Precision)  Difference(Recall)  \n",
       "0               1.376080           13.097623  \n",
       "1               8.026756           13.069054  \n",
       "2               8.414744           17.585569  \n",
       "3               1.443355           12.197856  \n",
       "4               6.462585            6.962785  \n",
       "5               5.810109            1.915709  \n",
       "6               0.533537            1.347882  \n",
       "7               2.439024            0.411783  \n",
       "8               5.396208            7.730934  \n",
       "9               4.112441            9.882006  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Decision Tree Classifier Cross Validation:\\n')\n",
    "f1_train = []\n",
    "f1_val = []\n",
    "precision_train = []\n",
    "precision_val = []\n",
    "recall_train = []\n",
    "recall_val = []\n",
    "random_state = []\n",
    "X = X_scaled\n",
    "y = df_train['Class']\n",
    "for i in range(0,10):\n",
    "    rs = random.randint(0, 100)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = rs)\n",
    "    test_model_dtc = DecisionTreeClassifier(splitter = 'best', min_samples_split = 59, min_samples_leaf = 16, max_depth = 45, criterion = 'gini')\n",
    "    test_model_dtc.fit(X_train, y_train)\n",
    "    y_train_pred_tdtc = test_model_dtc.predict(X_train)\n",
    "    y_val_pred_tdtc = test_model_dtc.predict(X_val)\n",
    "    random_state.append(rs)\n",
    "    f1_train.append(f1_score(y_train, y_train_pred_tdtc))\n",
    "    f1_val.append(f1_score(y_val, y_val_pred_tdtc))\n",
    "    precision_train.append(precision_score(y_train, y_train_pred_tdtc))\n",
    "    precision_val.append(precision_score(y_val, y_val_pred_tdtc))\n",
    "    recall_train.append(recall_score(y_train, y_train_pred_tdtc))\n",
    "    recall_val.append(recall_score(y_val, y_val_pred_tdtc))\n",
    "data = {'Random State' : random_state, 'Train f1' : f1_train, 'Validation f1' : f1_val, 'Train Precision' : precision_train, 'Validation Precision' : precision_val, \n",
    "        'Train Recall' : recall_train, 'Validation Recall' : recall_val}\n",
    "data = pd.DataFrame(data)\n",
    "data['Difference(f1)'] = ((np.abs(data['Train f1'] - data['Validation f1'])) * 100)/(data['Train f1'])\n",
    "data['Difference(Precision)'] = ((np.abs(data['Train Precision'] - data['Validation Precision'])) * 100)/(data['Train Precision'])\n",
    "data['Difference(Recall)'] = ((np.abs(data['Train Recall'] - data['Validation Recall'])) * 100)/(data['Train Recall'])\n",
    "print('Mean Difference(f1): ', data['Difference(f1)'].mean())\n",
    "print('Mean Difference(Precision): ', data['Difference(Precision)'].mean())\n",
    "print('Mean Difference(Recall): ', data['Difference(Recall)'].mean())\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:36:59.664046Z",
     "start_time": "2021-04-18T18:36:59.641894Z"
    }
   },
   "outputs": [],
   "source": [
    "#print('Random Forest Classifier Cross Validation:\\n')\n",
    "#f1_train = []\n",
    "#f1_val = []\n",
    "#precision_train = []\n",
    "#precision_val = []\n",
    "#recall_train = []\n",
    "#recall_val = []\n",
    "#random_state = []\n",
    "#X = X_scaled\n",
    "#y = df_train['Class']\n",
    "#for i in range(0, 10):\n",
    "#    rs = random.randint(0, 100)\n",
    "#    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = rs)\n",
    "#    test_model_rfc = RandomForestClassifier()\n",
    "#    test_model_rfc.fit(X_train, y_train)\n",
    "#    y_train_pred_trfc = test_model_rfc.predict(X_train)\n",
    "#    y_val_pred_trfc = test_model_rfc.predict(X_val)\n",
    "#    random_state.append(rs)\n",
    "#    f1_train.append(f1_score(y_train, y_train_pred_trfc))\n",
    "#    f1_val.append(f1_score(y_val, y_val_pred_trfc))\n",
    "#    precision_train.append(precision_score(y_train, y_train_pred_trfc))\n",
    "#    precision_val.append(precision_score(y_val, y_val_pred_trfc))\n",
    "#    recall_train.append(recall_score(y_train, y_train_pred_trfc))\n",
    "#    recall_val.append(recall_score(y_val, y_val_pred_trfc))\n",
    "#data = {'Random State' : random_state, 'Train f1' : f1_train, 'Validation f1' : f1_val, 'Train Precision' : precision_train, 'Validation Precision' : precision_val, \n",
    "#        'Train Recall' : recall_train, 'Validation Recall' : recall_val}\n",
    "#data = pd.DataFrame(data)\n",
    "#data['Difference(f1)'] = ((np.abs(data['Train f1'] - data['Validation f1'])) * 100)/(data['Train f1'])\n",
    "#data['Difference(Precision)'] = ((np.abs(data['Train Precision'] - data['Validation Precision'])) * 100)/(data['Train Precision'])\n",
    "#data['Difference(Recall)'] = ((np.abs(data['Train Recall'] - data['Validation Recall'])) * 100)/(data['Train Recall'])\n",
    "#print('Mean Difference(f1): ', data['Difference(f1)'].mean())\n",
    "#print('Mean Difference(Precision): ', data['Difference(Precision)'].mean())\n",
    "#print('Mean Difference(Recall): ', data['Difference(Recall)'].mean())\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:38:43.274844Z",
     "start_time": "2021-04-18T18:38:43.259307Z"
    }
   },
   "outputs": [],
   "source": [
    "#print('Ada Boost Classifier Cross Validation:\\n')\n",
    "#f1_train = []\n",
    "#f1_val = []\n",
    "#precision_train = []\n",
    "#precision_val = []\n",
    "#recall_train = []\n",
    "#recall_val = []\n",
    "#random_state = []\n",
    "#X = X_scaled\n",
    "#y = df_train['Class']\n",
    "#for i in range(0, 10):\n",
    "#    rs = random.randint(0, 100)\n",
    "#    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = rs)\n",
    "#    test_model_abc = AdaBoostClassifier()\n",
    "#    test_model_abc.fit(X_train, y_train)\n",
    "#    y_train_pred_tabc = test_model_abc.predict(X_train)\n",
    "#    y_val_pred_tabc = test_model_abc.predict(X_val)\n",
    "#    random_state.append(rs)\n",
    "#    f1_train.append(f1_score(y_train, y_train_pred_tabc))\n",
    "#    f1_val.append(f1_score(y_val, y_val_pred_tabc))\n",
    "#    precision_train.append(precision_score(y_train, y_train_pred_tabc))\n",
    "#    precision_val.append(precision_score(y_val, y_val_pred_tabc))\n",
    "#    recall_train.append(recall_score(y_train, y_train_pred_tabc))\n",
    "#    recall_val.append(recall_score(y_val, y_val_pred_tabc))\n",
    "#data = {'Random State' : random_state, 'Train f1' : f1_train, 'Validation f1' : f1_val, 'Train Precision' : precision_train, 'Validation Precision' : precision_val, \n",
    "#        'Train Recall' : recall_train, 'Validation Recall' : recall_val}\n",
    "#data = pd.DataFrame(data)\n",
    "#data['Difference(f1)'] = ((np.abs(data['Train f1'] - data['Validation f1'])) * 100)/(data['Train f1'])\n",
    "#data['Difference(Precision)'] = ((np.abs(data['Train Precision'] - data['Validation Precision'])) * 100)/(data['Train Precision'])\n",
    "#data['Difference(Recall)'] = ((np.abs(data['Train Recall'] - data['Validation Recall'])) * 100)/(data['Train Recall'])\n",
    "#print('Mean Difference(f1): ', data['Difference(f1)'].mean())\n",
    "#print('Mean Difference(Precision): ', data['Difference(Precision)'].mean())\n",
    "#print('Mean Difference(Recall): ', data['Difference(Recall)'].mean())\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T18:39:59.929599Z",
     "start_time": "2021-04-18T18:39:59.923394Z"
    }
   },
   "outputs": [],
   "source": [
    "#print('Gradient Boosting Classifier Cross Validation:\\n')\n",
    "#f1_train = []\n",
    "#f1_val = []\n",
    "#precision_train = []\n",
    "#precision_val = []\n",
    "#recall_train = []\n",
    "#recall_val = []\n",
    "#random_state = []\n",
    "#X = X_scaled\n",
    "#y = df_train['Class']\n",
    "#for i in range(0, 10):\n",
    "#    rs = random.randint(0, 100)\n",
    "#    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = rs)\n",
    "#    test_model_gbc = GradientBoostingClassifier()\n",
    "#    test_model_gbc.fit(X_train, y_train)\n",
    "#    y_train_pred_tgbc = test_model_gbc.predict(X_train)\n",
    "#    y_val_pred_tgbc = test_model_gbc.predict(X_val)\n",
    "#    random_state.append(rs)\n",
    "#    f1_train.append(f1_score(y_train, y_train_pred_tgbc))\n",
    "#    f1_val.append(f1_score(y_val, y_val_pred_tgbc))\n",
    "#    precision_train.append(precision_score(y_train, y_train_pred_tgbc))\n",
    "#    precision_val.append(precision_score(y_val, y_val_pred_tgbc))\n",
    "#    recall_train.append(recall_score(y_train, y_train_pred_tgbc))\n",
    "#    recall_val.append(recall_score(y_val, y_val_pred_tgbc))\n",
    "#data = {'Random State' : random_state, 'Train f1' : f1_train, 'Validation f1' : f1_val, 'Train Precision' : precision_train, 'Validation Precision' : precision_val, \n",
    "#        'Train Recall' : recall_train, 'Validation Recall' : recall_val}\n",
    "#data = pd.DataFrame(data)\n",
    "#data['Difference(f1)'] = ((np.abs(data['Train f1'] - data['Validation f1'])) * 100)/(data['Train f1'])\n",
    "#data['Difference(Precision)'] = ((np.abs(data['Train Precision'] - data['Validation Precision'])) * 100)/(data['Train Precision'])\n",
    "#data['Difference(Recall)'] = ((np.abs(data['Train Recall'] - data['Validation Recall'])) * 100)/(data['Train Recall'])\n",
    "#print('Mean Difference(f1): ', data['Difference(f1)'].mean())\n",
    "#print('Mean Difference(Precision): ', data['Difference(Precision)'].mean())\n",
    "#print('Mean Difference(Recall): ', data['Difference(Recall)'].mean())\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T19:02:26.376142Z",
     "start_time": "2021-04-18T18:41:11.607136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier Cross Validation:\n",
      "\n",
      "[00:11:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:13:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:15:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:17:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:19:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:21:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:23:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:26:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:28:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:30:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Mean Difference(f1):  14.859504321769744\n",
      "Mean Difference(Precision):  5.623478428475858\n",
      "Mean Difference(Recall):  22.24666814686092\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Random State</th>\n",
       "      <th>Train f1</th>\n",
       "      <th>Validation f1</th>\n",
       "      <th>Train Precision</th>\n",
       "      <th>Validation Precision</th>\n",
       "      <th>Train Recall</th>\n",
       "      <th>Validation Recall</th>\n",
       "      <th>Difference(f1)</th>\n",
       "      <th>Difference(Precision)</th>\n",
       "      <th>Difference(Recall)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.878378</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.942029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.822785</td>\n",
       "      <td>12.162162</td>\n",
       "      <td>5.797101</td>\n",
       "      <td>17.721519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.839161</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.779221</td>\n",
       "      <td>16.083916</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>22.077922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.877698</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.968254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.802632</td>\n",
       "      <td>12.230216</td>\n",
       "      <td>3.174603</td>\n",
       "      <td>19.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.836879</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>16.312057</td>\n",
       "      <td>3.278689</td>\n",
       "      <td>26.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.698630</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>1.923077</td>\n",
       "      <td>30.136986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>93</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.969231</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.741176</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>3.076923</td>\n",
       "      <td>25.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.953846</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>16.216216</td>\n",
       "      <td>4.615385</td>\n",
       "      <td>25.301205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.871429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.924242</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>12.857143</td>\n",
       "      <td>7.575758</td>\n",
       "      <td>17.567568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.845238</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.898734</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.797753</td>\n",
       "      <td>15.476190</td>\n",
       "      <td>10.126582</td>\n",
       "      <td>20.224719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.871429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.924242</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>12.857143</td>\n",
       "      <td>7.575758</td>\n",
       "      <td>17.567568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Random State  Train f1  Validation f1  Train Precision  \\\n",
       "0            50       1.0       0.878378              1.0   \n",
       "1            98       1.0       0.839161              1.0   \n",
       "2             9       1.0       0.877698              1.0   \n",
       "3             2       1.0       0.836879              1.0   \n",
       "4            16       1.0       0.816000              1.0   \n",
       "5            93       1.0       0.840000              1.0   \n",
       "6            30       1.0       0.837838              1.0   \n",
       "7            12       1.0       0.871429              1.0   \n",
       "8            26       1.0       0.845238              1.0   \n",
       "9            12       1.0       0.871429              1.0   \n",
       "\n",
       "   Validation Precision  Train Recall  Validation Recall  Difference(f1)  \\\n",
       "0              0.942029           1.0           0.822785       12.162162   \n",
       "1              0.909091           1.0           0.779221       16.083916   \n",
       "2              0.968254           1.0           0.802632       12.230216   \n",
       "3              0.967213           1.0           0.737500       16.312057   \n",
       "4              0.980769           1.0           0.698630       18.400000   \n",
       "5              0.969231           1.0           0.741176       16.000000   \n",
       "6              0.953846           1.0           0.746988       16.216216   \n",
       "7              0.924242           1.0           0.824324       12.857143   \n",
       "8              0.898734           1.0           0.797753       15.476190   \n",
       "9              0.924242           1.0           0.824324       12.857143   \n",
       "\n",
       "   Difference(Precision)  Difference(Recall)  \n",
       "0               5.797101           17.721519  \n",
       "1               9.090909           22.077922  \n",
       "2               3.174603           19.736842  \n",
       "3               3.278689           26.250000  \n",
       "4               1.923077           30.136986  \n",
       "5               3.076923           25.882353  \n",
       "6               4.615385           25.301205  \n",
       "7               7.575758           17.567568  \n",
       "8              10.126582           20.224719  \n",
       "9               7.575758           17.567568  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('XGBoost Classifier Cross Validation:\\n')\n",
    "f1_train = []\n",
    "f1_val = []\n",
    "precision_train = []\n",
    "precision_val = []\n",
    "recall_train = []\n",
    "recall_val = []\n",
    "random_state = []\n",
    "X = X_scaled\n",
    "y = df_train['Class']\n",
    "for i in range(0,10):\n",
    "    rs = random.randint(0, 100)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = rs)\n",
    "    test_model_xgbc = XGBClassifier(use_label_encoder = True, n_estimators = 750, max_depth = 75, learning_rate = 0.1)\n",
    "    test_model_xgbc.fit(X_train, y_train)\n",
    "    y_train_pred_txgbc = test_model_xgbc.predict(X_train)\n",
    "    y_val_pred_txgbc = test_model_xgbc.predict(X_val)\n",
    "    random_state.append(rs)\n",
    "    f1_train.append(f1_score(y_train, y_train_pred_txgbc))\n",
    "    f1_val.append(f1_score(y_val, y_val_pred_txgbc))\n",
    "    precision_train.append(precision_score(y_train, y_train_pred_txgbc))\n",
    "    precision_val.append(precision_score(y_val, y_val_pred_txgbc))\n",
    "    recall_train.append(recall_score(y_train, y_train_pred_txgbc))\n",
    "    recall_val.append(recall_score(y_val, y_val_pred_txgbc))\n",
    "data = {'Random State' : random_state, 'Train f1' : f1_train, 'Validation f1' : f1_val, 'Train Precision' : precision_train, 'Validation Precision' : precision_val, \n",
    "        'Train Recall' : recall_train, 'Validation Recall' : recall_val}\n",
    "data = pd.DataFrame(data)\n",
    "data['Difference(f1)'] = ((np.abs(data['Train f1'] - data['Validation f1'])) * 100)/(data['Train f1'])\n",
    "data['Difference(Precision)'] = ((np.abs(data['Train Precision'] - data['Validation Precision'])) * 100)/(data['Train Precision'])\n",
    "data['Difference(Recall)'] = ((np.abs(data['Train Recall'] - data['Validation Recall'])) * 100)/(data['Train Recall'])\n",
    "print('Mean Difference(f1): ', data['Difference(f1)'].mean())\n",
    "print('Mean Difference(Precision): ', data['Difference(Precision)'].mean())\n",
    "print('Mean Difference(Recall): ', data['Difference(Recall)'].mean())\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T07:50:54.444446Z",
     "start_time": "2021-04-19T07:50:54.398538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transaction ID</th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>468015</td>\n",
       "      <td>162104.0</td>\n",
       "      <td>1.352000</td>\n",
       "      <td>-1.673506</td>\n",
       "      <td>-0.636893</td>\n",
       "      <td>-0.336119</td>\n",
       "      <td>-1.542837</td>\n",
       "      <td>-1.226363</td>\n",
       "      <td>-0.124746</td>\n",
       "      <td>-0.227841</td>\n",
       "      <td>1.540701</td>\n",
       "      <td>-0.534952</td>\n",
       "      <td>-0.334849</td>\n",
       "      <td>0.228246</td>\n",
       "      <td>-0.664118</td>\n",
       "      <td>0.258531</td>\n",
       "      <td>1.259173</td>\n",
       "      <td>0.246986</td>\n",
       "      <td>-0.286924</td>\n",
       "      <td>-0.275878</td>\n",
       "      <td>-0.000941</td>\n",
       "      <td>0.465613</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>-0.558497</td>\n",
       "      <td>0.169942</td>\n",
       "      <td>0.466874</td>\n",
       "      <td>-0.853620</td>\n",
       "      <td>0.271841</td>\n",
       "      <td>-0.108573</td>\n",
       "      <td>0.016582</td>\n",
       "      <td>350.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>281092</td>\n",
       "      <td>57805.0</td>\n",
       "      <td>-0.768612</td>\n",
       "      <td>1.243045</td>\n",
       "      <td>1.091212</td>\n",
       "      <td>0.193316</td>\n",
       "      <td>-0.492666</td>\n",
       "      <td>-0.892758</td>\n",
       "      <td>0.219895</td>\n",
       "      <td>0.530861</td>\n",
       "      <td>-0.408229</td>\n",
       "      <td>-0.746581</td>\n",
       "      <td>0.115501</td>\n",
       "      <td>-0.352875</td>\n",
       "      <td>-1.236115</td>\n",
       "      <td>0.206609</td>\n",
       "      <td>1.210958</td>\n",
       "      <td>0.119147</td>\n",
       "      <td>0.722636</td>\n",
       "      <td>-0.560869</td>\n",
       "      <td>-0.647259</td>\n",
       "      <td>-0.116070</td>\n",
       "      <td>-0.170618</td>\n",
       "      <td>-0.594718</td>\n",
       "      <td>0.165624</td>\n",
       "      <td>0.517823</td>\n",
       "      <td>-0.321773</td>\n",
       "      <td>0.075277</td>\n",
       "      <td>0.126837</td>\n",
       "      <td>0.035620</td>\n",
       "      <td>9.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>212503</td>\n",
       "      <td>16781.0</td>\n",
       "      <td>-1.596231</td>\n",
       "      <td>0.404615</td>\n",
       "      <td>2.194786</td>\n",
       "      <td>-2.741233</td>\n",
       "      <td>-0.865271</td>\n",
       "      <td>-0.944760</td>\n",
       "      <td>-0.127869</td>\n",
       "      <td>0.502866</td>\n",
       "      <td>2.571508</td>\n",
       "      <td>-2.775742</td>\n",
       "      <td>0.868098</td>\n",
       "      <td>-1.375156</td>\n",
       "      <td>1.839486</td>\n",
       "      <td>1.269027</td>\n",
       "      <td>-0.387818</td>\n",
       "      <td>-0.322163</td>\n",
       "      <td>0.686652</td>\n",
       "      <td>-0.158159</td>\n",
       "      <td>-1.594824</td>\n",
       "      <td>-0.104222</td>\n",
       "      <td>0.110902</td>\n",
       "      <td>0.596544</td>\n",
       "      <td>-0.219749</td>\n",
       "      <td>0.600184</td>\n",
       "      <td>0.598460</td>\n",
       "      <td>-0.255279</td>\n",
       "      <td>0.143856</td>\n",
       "      <td>0.019840</td>\n",
       "      <td>37.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>367732</td>\n",
       "      <td>117554.0</td>\n",
       "      <td>-0.382983</td>\n",
       "      <td>1.063542</td>\n",
       "      <td>-1.094111</td>\n",
       "      <td>-0.053626</td>\n",
       "      <td>0.537947</td>\n",
       "      <td>-1.388576</td>\n",
       "      <td>0.663167</td>\n",
       "      <td>-0.216840</td>\n",
       "      <td>0.372559</td>\n",
       "      <td>-0.476902</td>\n",
       "      <td>-0.598992</td>\n",
       "      <td>-0.475948</td>\n",
       "      <td>-0.520418</td>\n",
       "      <td>-0.884354</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.117423</td>\n",
       "      <td>0.586687</td>\n",
       "      <td>0.692713</td>\n",
       "      <td>-0.254871</td>\n",
       "      <td>-0.448479</td>\n",
       "      <td>0.271541</td>\n",
       "      <td>0.955643</td>\n",
       "      <td>0.072718</td>\n",
       "      <td>-0.176539</td>\n",
       "      <td>-0.311954</td>\n",
       "      <td>-0.205355</td>\n",
       "      <td>-0.691227</td>\n",
       "      <td>-0.324247</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>480118</td>\n",
       "      <td>167993.0</td>\n",
       "      <td>1.911940</td>\n",
       "      <td>0.109419</td>\n",
       "      <td>-0.379744</td>\n",
       "      <td>3.381940</td>\n",
       "      <td>0.281194</td>\n",
       "      <td>1.138123</td>\n",
       "      <td>-0.670080</td>\n",
       "      <td>0.386373</td>\n",
       "      <td>-0.382842</td>\n",
       "      <td>1.654088</td>\n",
       "      <td>-1.030125</td>\n",
       "      <td>-0.943646</td>\n",
       "      <td>-1.370587</td>\n",
       "      <td>0.361032</td>\n",
       "      <td>-0.484696</td>\n",
       "      <td>2.076593</td>\n",
       "      <td>-1.742429</td>\n",
       "      <td>1.271893</td>\n",
       "      <td>-1.076403</td>\n",
       "      <td>-0.305569</td>\n",
       "      <td>0.143581</td>\n",
       "      <td>0.317334</td>\n",
       "      <td>0.038104</td>\n",
       "      <td>-1.465384</td>\n",
       "      <td>-0.189313</td>\n",
       "      <td>0.067137</td>\n",
       "      <td>-0.009806</td>\n",
       "      <td>-0.056292</td>\n",
       "      <td>15.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Transaction ID      Time        V1        V2        V3        V4        V5  \\\n",
       "0          468015  162104.0  1.352000 -1.673506 -0.636893 -0.336119 -1.542837   \n",
       "1          281092   57805.0 -0.768612  1.243045  1.091212  0.193316 -0.492666   \n",
       "2          212503   16781.0 -1.596231  0.404615  2.194786 -2.741233 -0.865271   \n",
       "3          367732  117554.0 -0.382983  1.063542 -1.094111 -0.053626  0.537947   \n",
       "4          480118  167993.0  1.911940  0.109419 -0.379744  3.381940  0.281194   \n",
       "\n",
       "         V6        V7        V8        V9       V10       V11       V12  \\\n",
       "0 -1.226363 -0.124746 -0.227841  1.540701 -0.534952 -0.334849  0.228246   \n",
       "1 -0.892758  0.219895  0.530861 -0.408229 -0.746581  0.115501 -0.352875   \n",
       "2 -0.944760 -0.127869  0.502866  2.571508 -2.775742  0.868098 -1.375156   \n",
       "3 -1.388576  0.663167 -0.216840  0.372559 -0.476902 -0.598992 -0.475948   \n",
       "4  1.138123 -0.670080  0.386373 -0.382842  1.654088 -1.030125 -0.943646   \n",
       "\n",
       "        V13       V14       V15       V16       V17       V18       V19  \\\n",
       "0 -0.664118  0.258531  1.259173  0.246986 -0.286924 -0.275878 -0.000941   \n",
       "1 -1.236115  0.206609  1.210958  0.119147  0.722636 -0.560869 -0.647259   \n",
       "2  1.839486  1.269027 -0.387818 -0.322163  0.686652 -0.158159 -1.594824   \n",
       "3 -0.520418 -0.884354  0.854545  0.117423  0.586687  0.692713 -0.254871   \n",
       "4 -1.370587  0.361032 -0.484696  2.076593 -1.742429  1.271893 -1.076403   \n",
       "\n",
       "        V20       V21       V22       V23       V24       V25       V26  \\\n",
       "0  0.465613  0.051400 -0.558497  0.169942  0.466874 -0.853620  0.271841   \n",
       "1 -0.116070 -0.170618 -0.594718  0.165624  0.517823 -0.321773  0.075277   \n",
       "2 -0.104222  0.110902  0.596544 -0.219749  0.600184  0.598460 -0.255279   \n",
       "3 -0.448479  0.271541  0.955643  0.072718 -0.176539 -0.311954 -0.205355   \n",
       "4 -0.305569  0.143581  0.317334  0.038104 -1.465384 -0.189313  0.067137   \n",
       "\n",
       "        V27       V28  Amount  Class  \n",
       "0 -0.108573  0.016582  350.00      0  \n",
       "1  0.126837  0.035620    9.81      0  \n",
       "2  0.143856  0.019840   37.08      0  \n",
       "3 -0.691227 -0.324247    2.99      0  \n",
       "4 -0.009806 -0.056292   15.17      0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = df_test\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T07:50:54.508311Z",
     "start_time": "2021-04-19T07:50:54.446435Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = df_test.drop(['Transaction ID', 'Time', 'Class'], axis = 1)\n",
    "y_test = df_test['Class']\n",
    "X_test = pd.DataFrame(ss.fit_transform(X_test), columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T07:50:54.556192Z",
     "start_time": "2021-04-19T07:50:54.509311Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.691631</td>\n",
       "      <td>-1.005289</td>\n",
       "      <td>-0.425668</td>\n",
       "      <td>-0.238792</td>\n",
       "      <td>-1.129568</td>\n",
       "      <td>-0.925052</td>\n",
       "      <td>-0.101540</td>\n",
       "      <td>-0.194625</td>\n",
       "      <td>1.412383</td>\n",
       "      <td>-0.494863</td>\n",
       "      <td>-0.329989</td>\n",
       "      <td>0.225945</td>\n",
       "      <td>-0.669302</td>\n",
       "      <td>0.266872</td>\n",
       "      <td>1.376178</td>\n",
       "      <td>0.279270</td>\n",
       "      <td>-0.330354</td>\n",
       "      <td>-0.326214</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>0.625142</td>\n",
       "      <td>0.064638</td>\n",
       "      <td>-0.770482</td>\n",
       "      <td>0.270978</td>\n",
       "      <td>0.775015</td>\n",
       "      <td>-1.634349</td>\n",
       "      <td>0.553621</td>\n",
       "      <td>-0.270696</td>\n",
       "      <td>0.044808</td>\n",
       "      <td>1.139044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.393768</td>\n",
       "      <td>0.750198</td>\n",
       "      <td>0.728660</td>\n",
       "      <td>0.135429</td>\n",
       "      <td>-0.360798</td>\n",
       "      <td>-0.673758</td>\n",
       "      <td>0.188685</td>\n",
       "      <td>0.445669</td>\n",
       "      <td>-0.367753</td>\n",
       "      <td>-0.689823</td>\n",
       "      <td>0.109962</td>\n",
       "      <td>-0.351717</td>\n",
       "      <td>-1.242926</td>\n",
       "      <td>0.213296</td>\n",
       "      <td>1.323625</td>\n",
       "      <td>0.134186</td>\n",
       "      <td>0.836916</td>\n",
       "      <td>-0.665130</td>\n",
       "      <td>-0.796009</td>\n",
       "      <td>-0.158424</td>\n",
       "      <td>-0.243576</td>\n",
       "      <td>-0.820449</td>\n",
       "      <td>0.263967</td>\n",
       "      <td>0.859594</td>\n",
       "      <td>-0.610631</td>\n",
       "      <td>0.147694</td>\n",
       "      <td>0.320351</td>\n",
       "      <td>0.099297</td>\n",
       "      <td>-0.341190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.817371</td>\n",
       "      <td>0.245543</td>\n",
       "      <td>1.465818</td>\n",
       "      <td>-1.938802</td>\n",
       "      <td>-0.633561</td>\n",
       "      <td>-0.712929</td>\n",
       "      <td>-0.104169</td>\n",
       "      <td>0.422043</td>\n",
       "      <td>2.353912</td>\n",
       "      <td>-2.559153</td>\n",
       "      <td>0.845180</td>\n",
       "      <td>-1.367915</td>\n",
       "      <td>1.841421</td>\n",
       "      <td>1.309564</td>\n",
       "      <td>-0.419008</td>\n",
       "      <td>-0.366656</td>\n",
       "      <td>0.795310</td>\n",
       "      <td>-0.186221</td>\n",
       "      <td>-1.964189</td>\n",
       "      <td>-0.142464</td>\n",
       "      <td>0.147240</td>\n",
       "      <td>0.822860</td>\n",
       "      <td>-0.361711</td>\n",
       "      <td>0.996319</td>\n",
       "      <td>1.160667</td>\n",
       "      <td>-0.534943</td>\n",
       "      <td>0.363080</td>\n",
       "      <td>0.054135</td>\n",
       "      <td>-0.222533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.196390</td>\n",
       "      <td>0.642154</td>\n",
       "      <td>-0.731078</td>\n",
       "      <td>-0.039117</td>\n",
       "      <td>0.393655</td>\n",
       "      <td>-1.047243</td>\n",
       "      <td>0.561969</td>\n",
       "      <td>-0.185342</td>\n",
       "      <td>0.345412</td>\n",
       "      <td>-0.441386</td>\n",
       "      <td>-0.588032</td>\n",
       "      <td>-0.474058</td>\n",
       "      <td>-0.525193</td>\n",
       "      <td>-0.912426</td>\n",
       "      <td>0.935141</td>\n",
       "      <td>0.132229</td>\n",
       "      <td>0.679729</td>\n",
       "      <td>0.825650</td>\n",
       "      <td>-0.312264</td>\n",
       "      <td>-0.606201</td>\n",
       "      <td>0.370246</td>\n",
       "      <td>1.318226</td>\n",
       "      <td>0.113127</td>\n",
       "      <td>-0.293092</td>\n",
       "      <td>-0.591732</td>\n",
       "      <td>-0.431843</td>\n",
       "      <td>-1.733571</td>\n",
       "      <td>-0.930676</td>\n",
       "      <td>-0.370865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.978227</td>\n",
       "      <td>0.067862</td>\n",
       "      <td>-0.253899</td>\n",
       "      <td>2.389249</td>\n",
       "      <td>0.205701</td>\n",
       "      <td>0.856043</td>\n",
       "      <td>-0.560770</td>\n",
       "      <td>0.323730</td>\n",
       "      <td>-0.344565</td>\n",
       "      <td>1.521753</td>\n",
       "      <td>-1.009209</td>\n",
       "      <td>-0.938972</td>\n",
       "      <td>-1.377780</td>\n",
       "      <td>0.372639</td>\n",
       "      <td>-0.524604</td>\n",
       "      <td>2.355686</td>\n",
       "      <td>-2.013229</td>\n",
       "      <td>1.514420</td>\n",
       "      <td>-1.325067</td>\n",
       "      <td>-0.413691</td>\n",
       "      <td>0.192607</td>\n",
       "      <td>0.437698</td>\n",
       "      <td>0.056930</td>\n",
       "      <td>-2.432661</td>\n",
       "      <td>-0.355668</td>\n",
       "      <td>0.130884</td>\n",
       "      <td>-0.022719</td>\n",
       "      <td>-0.163763</td>\n",
       "      <td>-0.317868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0  0.691631 -1.005289 -0.425668 -0.238792 -1.129568 -0.925052 -0.101540   \n",
       "1 -0.393768  0.750198  0.728660  0.135429 -0.360798 -0.673758  0.188685   \n",
       "2 -0.817371  0.245543  1.465818 -1.938802 -0.633561 -0.712929 -0.104169   \n",
       "3 -0.196390  0.642154 -0.731078 -0.039117  0.393655 -1.047243  0.561969   \n",
       "4  0.978227  0.067862 -0.253899  2.389249  0.205701  0.856043 -0.560770   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0 -0.194625  1.412383 -0.494863 -0.329989  0.225945 -0.669302  0.266872   \n",
       "1  0.445669 -0.367753 -0.689823  0.109962 -0.351717 -1.242926  0.213296   \n",
       "2  0.422043  2.353912 -2.559153  0.845180 -1.367915  1.841421  1.309564   \n",
       "3 -0.185342  0.345412 -0.441386 -0.588032 -0.474058 -0.525193 -0.912426   \n",
       "4  0.323730 -0.344565  1.521753 -1.009209 -0.938972 -1.377780  0.372639   \n",
       "\n",
       "        V15       V16       V17       V18       V19       V20       V21  \\\n",
       "0  1.376178  0.279270 -0.330354 -0.326214  0.000785  0.625142  0.064638   \n",
       "1  1.323625  0.134186  0.836916 -0.665130 -0.796009 -0.158424 -0.243576   \n",
       "2 -0.419008 -0.366656  0.795310 -0.186221 -1.964189 -0.142464  0.147240   \n",
       "3  0.935141  0.132229  0.679729  0.825650 -0.312264 -0.606201  0.370246   \n",
       "4 -0.524604  2.355686 -2.013229  1.514420 -1.325067 -0.413691  0.192607   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0 -0.770482  0.270978  0.775015 -1.634349  0.553621 -0.270696  0.044808   \n",
       "1 -0.820449  0.263967  0.859594 -0.610631  0.147694  0.320351  0.099297   \n",
       "2  0.822860 -0.361711  0.996319  1.160667 -0.534943  0.363080  0.054135   \n",
       "3  1.318226  0.113127 -0.293092 -0.591732 -0.431843 -1.733571 -0.930676   \n",
       "4  0.437698  0.056930 -2.432661 -0.355668  0.130884 -0.022719 -0.163763   \n",
       "\n",
       "     Amount  \n",
       "0  1.139044  \n",
       "1 -0.341190  \n",
       "2 -0.222533  \n",
       "3 -0.370865  \n",
       "4 -0.317868  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T07:50:54.603860Z",
     "start_time": "2021-04-19T07:50:54.558177Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transaction ID</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>468015</td>\n",
       "      <td>0.691631</td>\n",
       "      <td>-1.005289</td>\n",
       "      <td>-0.425668</td>\n",
       "      <td>-0.238792</td>\n",
       "      <td>-1.129568</td>\n",
       "      <td>-0.925052</td>\n",
       "      <td>-0.101540</td>\n",
       "      <td>-0.194625</td>\n",
       "      <td>1.412383</td>\n",
       "      <td>-0.494863</td>\n",
       "      <td>-0.329989</td>\n",
       "      <td>0.225945</td>\n",
       "      <td>-0.669302</td>\n",
       "      <td>0.266872</td>\n",
       "      <td>1.376178</td>\n",
       "      <td>0.279270</td>\n",
       "      <td>-0.330354</td>\n",
       "      <td>-0.326214</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>0.625142</td>\n",
       "      <td>0.064638</td>\n",
       "      <td>-0.770482</td>\n",
       "      <td>0.270978</td>\n",
       "      <td>0.775015</td>\n",
       "      <td>-1.634349</td>\n",
       "      <td>0.553621</td>\n",
       "      <td>-0.270696</td>\n",
       "      <td>0.044808</td>\n",
       "      <td>1.139044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>281092</td>\n",
       "      <td>-0.393768</td>\n",
       "      <td>0.750198</td>\n",
       "      <td>0.728660</td>\n",
       "      <td>0.135429</td>\n",
       "      <td>-0.360798</td>\n",
       "      <td>-0.673758</td>\n",
       "      <td>0.188685</td>\n",
       "      <td>0.445669</td>\n",
       "      <td>-0.367753</td>\n",
       "      <td>-0.689823</td>\n",
       "      <td>0.109962</td>\n",
       "      <td>-0.351717</td>\n",
       "      <td>-1.242926</td>\n",
       "      <td>0.213296</td>\n",
       "      <td>1.323625</td>\n",
       "      <td>0.134186</td>\n",
       "      <td>0.836916</td>\n",
       "      <td>-0.665130</td>\n",
       "      <td>-0.796009</td>\n",
       "      <td>-0.158424</td>\n",
       "      <td>-0.243576</td>\n",
       "      <td>-0.820449</td>\n",
       "      <td>0.263967</td>\n",
       "      <td>0.859594</td>\n",
       "      <td>-0.610631</td>\n",
       "      <td>0.147694</td>\n",
       "      <td>0.320351</td>\n",
       "      <td>0.099297</td>\n",
       "      <td>-0.341190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>212503</td>\n",
       "      <td>-0.817371</td>\n",
       "      <td>0.245543</td>\n",
       "      <td>1.465818</td>\n",
       "      <td>-1.938802</td>\n",
       "      <td>-0.633561</td>\n",
       "      <td>-0.712929</td>\n",
       "      <td>-0.104169</td>\n",
       "      <td>0.422043</td>\n",
       "      <td>2.353912</td>\n",
       "      <td>-2.559153</td>\n",
       "      <td>0.845180</td>\n",
       "      <td>-1.367915</td>\n",
       "      <td>1.841421</td>\n",
       "      <td>1.309564</td>\n",
       "      <td>-0.419008</td>\n",
       "      <td>-0.366656</td>\n",
       "      <td>0.795310</td>\n",
       "      <td>-0.186221</td>\n",
       "      <td>-1.964189</td>\n",
       "      <td>-0.142464</td>\n",
       "      <td>0.147240</td>\n",
       "      <td>0.822860</td>\n",
       "      <td>-0.361711</td>\n",
       "      <td>0.996319</td>\n",
       "      <td>1.160667</td>\n",
       "      <td>-0.534943</td>\n",
       "      <td>0.363080</td>\n",
       "      <td>0.054135</td>\n",
       "      <td>-0.222533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>367732</td>\n",
       "      <td>-0.196390</td>\n",
       "      <td>0.642154</td>\n",
       "      <td>-0.731078</td>\n",
       "      <td>-0.039117</td>\n",
       "      <td>0.393655</td>\n",
       "      <td>-1.047243</td>\n",
       "      <td>0.561969</td>\n",
       "      <td>-0.185342</td>\n",
       "      <td>0.345412</td>\n",
       "      <td>-0.441386</td>\n",
       "      <td>-0.588032</td>\n",
       "      <td>-0.474058</td>\n",
       "      <td>-0.525193</td>\n",
       "      <td>-0.912426</td>\n",
       "      <td>0.935141</td>\n",
       "      <td>0.132229</td>\n",
       "      <td>0.679729</td>\n",
       "      <td>0.825650</td>\n",
       "      <td>-0.312264</td>\n",
       "      <td>-0.606201</td>\n",
       "      <td>0.370246</td>\n",
       "      <td>1.318226</td>\n",
       "      <td>0.113127</td>\n",
       "      <td>-0.293092</td>\n",
       "      <td>-0.591732</td>\n",
       "      <td>-0.431843</td>\n",
       "      <td>-1.733571</td>\n",
       "      <td>-0.930676</td>\n",
       "      <td>-0.370865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>480118</td>\n",
       "      <td>0.978227</td>\n",
       "      <td>0.067862</td>\n",
       "      <td>-0.253899</td>\n",
       "      <td>2.389249</td>\n",
       "      <td>0.205701</td>\n",
       "      <td>0.856043</td>\n",
       "      <td>-0.560770</td>\n",
       "      <td>0.323730</td>\n",
       "      <td>-0.344565</td>\n",
       "      <td>1.521753</td>\n",
       "      <td>-1.009209</td>\n",
       "      <td>-0.938972</td>\n",
       "      <td>-1.377780</td>\n",
       "      <td>0.372639</td>\n",
       "      <td>-0.524604</td>\n",
       "      <td>2.355686</td>\n",
       "      <td>-2.013229</td>\n",
       "      <td>1.514420</td>\n",
       "      <td>-1.325067</td>\n",
       "      <td>-0.413691</td>\n",
       "      <td>0.192607</td>\n",
       "      <td>0.437698</td>\n",
       "      <td>0.056930</td>\n",
       "      <td>-2.432661</td>\n",
       "      <td>-0.355668</td>\n",
       "      <td>0.130884</td>\n",
       "      <td>-0.022719</td>\n",
       "      <td>-0.163763</td>\n",
       "      <td>-0.317868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Transaction ID        V1        V2        V3        V4        V5        V6  \\\n",
       "0          468015  0.691631 -1.005289 -0.425668 -0.238792 -1.129568 -0.925052   \n",
       "1          281092 -0.393768  0.750198  0.728660  0.135429 -0.360798 -0.673758   \n",
       "2          212503 -0.817371  0.245543  1.465818 -1.938802 -0.633561 -0.712929   \n",
       "3          367732 -0.196390  0.642154 -0.731078 -0.039117  0.393655 -1.047243   \n",
       "4          480118  0.978227  0.067862 -0.253899  2.389249  0.205701  0.856043   \n",
       "\n",
       "         V7        V8        V9       V10       V11       V12       V13  \\\n",
       "0 -0.101540 -0.194625  1.412383 -0.494863 -0.329989  0.225945 -0.669302   \n",
       "1  0.188685  0.445669 -0.367753 -0.689823  0.109962 -0.351717 -1.242926   \n",
       "2 -0.104169  0.422043  2.353912 -2.559153  0.845180 -1.367915  1.841421   \n",
       "3  0.561969 -0.185342  0.345412 -0.441386 -0.588032 -0.474058 -0.525193   \n",
       "4 -0.560770  0.323730 -0.344565  1.521753 -1.009209 -0.938972 -1.377780   \n",
       "\n",
       "        V14       V15       V16       V17       V18       V19       V20  \\\n",
       "0  0.266872  1.376178  0.279270 -0.330354 -0.326214  0.000785  0.625142   \n",
       "1  0.213296  1.323625  0.134186  0.836916 -0.665130 -0.796009 -0.158424   \n",
       "2  1.309564 -0.419008 -0.366656  0.795310 -0.186221 -1.964189 -0.142464   \n",
       "3 -0.912426  0.935141  0.132229  0.679729  0.825650 -0.312264 -0.606201   \n",
       "4  0.372639 -0.524604  2.355686 -2.013229  1.514420 -1.325067 -0.413691   \n",
       "\n",
       "        V21       V22       V23       V24       V25       V26       V27  \\\n",
       "0  0.064638 -0.770482  0.270978  0.775015 -1.634349  0.553621 -0.270696   \n",
       "1 -0.243576 -0.820449  0.263967  0.859594 -0.610631  0.147694  0.320351   \n",
       "2  0.147240  0.822860 -0.361711  0.996319  1.160667 -0.534943  0.363080   \n",
       "3  0.370246  1.318226  0.113127 -0.293092 -0.591732 -0.431843 -1.733571   \n",
       "4  0.192607  0.437698  0.056930 -2.432661 -0.355668  0.130884 -0.022719   \n",
       "\n",
       "        V28    Amount  \n",
       "0  0.044808  1.139044  \n",
       "1  0.099297 -0.341190  \n",
       "2  0.054135 -0.222533  \n",
       "3 -0.930676 -0.370865  \n",
       "4 -0.163763 -0.317868  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.concat([df_test['Transaction ID'], X_test], axis = 1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T07:50:55.347962Z",
     "start_time": "2021-04-19T07:50:55.315525Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=45, min_samples_leaf=16, min_samples_split=59)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model_dtc = DecisionTreeClassifier(splitter = 'best', min_samples_split = 59, min_samples_leaf = 16, max_depth = 45, criterion = 'gini')\n",
    "test_model_dtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T07:51:05.497638Z",
     "start_time": "2021-04-19T07:50:56.163165Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=45, min_samples_leaf=16, min_samples_split=59)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model_dtc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T07:54:22.912832Z",
     "start_time": "2021-04-19T07:54:22.898348Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300315"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txn_id = 300315\n",
    "txn_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T07:54:35.562145Z",
     "start_time": "2021-04-19T07:54:35.475377Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transaction ID</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>300315</td>\n",
       "      <td>0.690519</td>\n",
       "      <td>-0.295924</td>\n",
       "      <td>0.511767</td>\n",
       "      <td>-0.447516</td>\n",
       "      <td>-0.84804</td>\n",
       "      <td>-0.522602</td>\n",
       "      <td>-0.611711</td>\n",
       "      <td>-0.056095</td>\n",
       "      <td>-0.664462</td>\n",
       "      <td>0.523955</td>\n",
       "      <td>0.299507</td>\n",
       "      <td>-0.285387</td>\n",
       "      <td>0.332139</td>\n",
       "      <td>-0.230764</td>\n",
       "      <td>1.17999</td>\n",
       "      <td>0.982911</td>\n",
       "      <td>0.658941</td>\n",
       "      <td>-2.263069</td>\n",
       "      <td>-0.004856</td>\n",
       "      <td>0.035727</td>\n",
       "      <td>0.205301</td>\n",
       "      <td>0.574259</td>\n",
       "      <td>0.144323</td>\n",
       "      <td>0.682422</td>\n",
       "      <td>0.500528</td>\n",
       "      <td>-0.571281</td>\n",
       "      <td>0.091338</td>\n",
       "      <td>0.055553</td>\n",
       "      <td>-0.375173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Transaction ID        V1        V2        V3        V4       V5        V6  \\\n",
       "6          300315  0.690519 -0.295924  0.511767 -0.447516 -0.84804 -0.522602   \n",
       "\n",
       "         V7        V8        V9       V10       V11       V12       V13  \\\n",
       "6 -0.611711 -0.056095 -0.664462  0.523955  0.299507 -0.285387  0.332139   \n",
       "\n",
       "        V14      V15       V16       V17       V18       V19       V20  \\\n",
       "6 -0.230764  1.17999  0.982911  0.658941 -2.263069 -0.004856  0.035727   \n",
       "\n",
       "        V21       V22       V23       V24       V25       V26       V27  \\\n",
       "6  0.205301  0.574259  0.144323  0.682422  0.500528 -0.571281  0.091338   \n",
       "\n",
       "        V28    Amount  \n",
       "6  0.055553 -0.375173  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data = X_test[X_test['Transaction ID'] == txn_id]\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:01:54.636328Z",
     "start_time": "2021-04-19T08:01:54.618850Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = test_model_dtc.predict_proba(filtered_data.iloc[:, 1:])[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:01:55.226374Z",
     "start_time": "2021-04-19T08:01:55.218883Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T19:11:50.501724Z",
     "start_time": "2021-04-18T19:11:50.467310Z"
    }
   },
   "outputs": [],
   "source": [
    "file = open('fraud_detect.pkl', 'wb')\n",
    "pickle.dump(test_model_dtc, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
